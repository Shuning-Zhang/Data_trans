{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE_COT= '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "\n",
    "    input dataset: {input_list} \n",
    "\n",
    "    output dataset: {output_list}\n",
    "\n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "\n",
    "    You should think step by step how the transformation was performed.\n",
    "\n",
    "    you should generate python code to reproduce the transformation.\n",
    "    \n",
    "    In the python code only print out the transformed dataset, no other information.\n",
    "\n",
    "    you are also given a test set {test_list} where you should include the test set in your python code\n",
    "\n",
    "    '''\n",
    "\n",
    "TEMPLATE_C= '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "        \n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "    \n",
    "    Generate python function with no explanations needed. \n",
    "        \n",
    "    input dataset: {input_list} \n",
    "    output dataset: {output_list}\n",
    "    \n",
    "    you are also given a test set {test_list} where you should include the test set in your python code as the function input\n",
    "    Do not include input database in your code output\n",
    "    \n",
    "    '''\n",
    "TEMPLATE_6= '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "        \n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "    \n",
    "    Generate python function with no explanations needed. \n",
    "        \n",
    "    input dataset: {input_list} \n",
    "    output dataset: {output_list}\n",
    "    \n",
    "    you are also given a test set {test_list} where you should include the test set in your python code as the function input\n",
    "    Do not include input database in your code output\n",
    "\n",
    "    Note: The element from first row of input dataset will be the first element in each row of the output, follows by product\n",
    "    \n",
    "    '''\n",
    "TEMPLATE_26= '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "        \n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "    \n",
    "    Generate python function with no explanations needed. \n",
    "        \n",
    "    input dataset: {input_list} \n",
    "    output dataset: {output_list}\n",
    "    \n",
    "    you are also given a test set {test_list} where you should include the test set in your python code as the function input\n",
    "    Do not include input database in your code output\n",
    "\n",
    "    Note: change from 1 element a sublist to 4 element per sublist\n",
    "    \n",
    "    '''\n",
    "TEMPLATE_29= '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "        \n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "    \n",
    "    Generate python function with no explanations needed. \n",
    "        \n",
    "    input dataset: {input_list} \n",
    "    output dataset: {output_list}\n",
    "    \n",
    "    you are also given a test set {test_list} where you should include the test set in your python code as the function input\n",
    "    Do not include input database in your code output\n",
    "\n",
    "    Note: combine 2 two from the input to 1 row for the output, but only keep the first word in the first row and all the numerical values\n",
    "    \n",
    "    '''\n",
    "TEMPLATE_LANGUAGE = '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "        \n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "    \n",
    "    Generate python function with no explanations needed. \n",
    "        \n",
    "    input dataset: {input_list} \n",
    "    output dataset: {output_list}\n",
    "    \n",
    "    you are also given a test set {test_list} where you should include the test set in your python code as the function input\n",
    "    Do not include input database in your code output\n",
    "\n",
    "    Note: You should use information you have with this problem such as countries respective ISO language code eg [\"Arabic = ar\"].\n",
    "    \n",
    "    '''\n",
    "TEMPLATE_KG= '''\n",
    "\n",
    "    Given an example input and output dataset, learn how the transformation performed from the provided input dataset to the output dataset. \n",
    "    Common operations in data cleaning and data wrangling include: identifying and removing duplicate data, handling missing values, \n",
    "    filtering outliers, standardizing data formats, data type conversion, data aggregation, joining datasets, data validation, \n",
    "    and data enrichment by adding information from external sources. Note you should not use the datetime package.\n",
    "        \n",
    "    Pay attention to the size difference between the input and output dataset.\n",
    "    \n",
    "    Generate python function with no explanations needed. \n",
    "        \n",
    "    input dataset: {input_list} \n",
    "    output dataset: {output_list}\n",
    "    \n",
    "    you are also given a test set {test_list} where you should include the test set in your python code as the function input\n",
    "    Do not include input database in your code output\n",
    "    '''\n",
    "TEMPLATE_KG2= '''\n",
    "    Given an example input and output dataset, learn the transformation applied to convert the provided input dataset into the output dataset.\n",
    "\n",
    "    Data transformations can include various cleaning and wrangling operations, such as identifying and removing duplicate data, \n",
    "    handling missing values, filtering outliers, standardizing data formats, performing data type conversion, data aggregation, \n",
    "    joining datasets, validating data integrity, or enriching the dataset by adding information from external sources. \n",
    "    However, avoid using any datetime-specific packages for this task.\n",
    "\n",
    "    When analyzing the transformation, take note of potential changes in the dataset's structure, size, or format between input and output.\n",
    "\n",
    "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
    "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
    "    \n",
    "    input dataset: {input_list}\n",
    "    output dataset: {output_list}\n",
    "    test set: {test_list}\n",
    "    '''\n",
    "TEMPLATE_KG_LONG = ''' \n",
    "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. Here are some common transformations to consider, along with examples where appropriate:\n",
    "\n",
    "    Removing Duplicate Data: Detect and remove repeated entries based on specific columns or across the entire dataset.\n",
    "    Example: If the input dataset contains multiple rows for the same ID, the output dataset may include only the first instance.\n",
    "\n",
    "    Handling Missing Values: Fill or remove missing data based on the output dataset. You may need to impute values with the mean, median, or mode, or drop rows/columns entirely if missing data is excessive.\n",
    "    Example: If the input dataset has rows with missing ages, the output dataset might replace them with the average age.\n",
    "\n",
    "    Filtering Outliers: Remove or modify extreme values that are beyond an acceptable range.\n",
    "    Example: If a numerical column has values that are three standard deviations away from the mean, they might be removed or capped at a specific threshold.\n",
    "\n",
    "    Standardizing Data Formats: Ensure consistency in text formats, date formats, and number formats.\n",
    "    Example: Converting all names to title case (e.g., \"john doe\" to \"John Doe\") or ensuring all dates follow the \"YYYY-MM-DD\" format.\n",
    "\n",
    "    Data Type Conversion: Change column data types as required by the output dataset, such as converting strings to numbers or dates to strings.\n",
    "    Example: If a column contains numeric values stored as strings in the input dataset, the output dataset might have them converted to integers or floats.\n",
    "\n",
    "    Data Aggregation: Combine data at a higher level, like summing, averaging, or grouping by a specific attribute.\n",
    "    Example: If the input dataset contains daily sales, the output dataset might aggregate these values to show monthly or weekly totals.\n",
    "\n",
    "    Joining Datasets: Merge data from additional sources if the output dataset contains extra columns not present in the input dataset.\n",
    "    Example: Joining a product details dataset with the input dataset based on a product ID to include product names in the output.\n",
    "\n",
    "    Take note of changes in size, format, or structure between the input and output datasets. Also, ensure that any transformations are consistent with observed changes across multiple examples if provided.\n",
    "\n",
    "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
    "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
    "    \n",
    "    input dataset: {input_list}\n",
    "    output dataset: {output_list}\n",
    "    test set: {test_list}\n",
    "    '''\n",
    "TEMPLATE_WITH_FUNC = '''\n",
    "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. \n",
    "    These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. \n",
    "    Here are some common transformations to consider, along with examples where appropriate, some sample reference functions are provided but you are not limited or have to use the provided functions:\n",
    "\n",
    "    # Combine the first two rows into one by concatenation\n",
    "    def combine_first_two_rows(data):\n",
    "        \"\"\"\n",
    "        Combines the first two rows in a 2D list by concatenating them.\n",
    "        Returns a new 2D list with the combined row.\n",
    "        \"\"\"\n",
    "        if len(data) < 2:\n",
    "            raise ValueError(\"Not enough rows to combine.\")\n",
    "        new_row = data[0] + data[1]\n",
    "        new_data = [new_row] + data[2:]\n",
    "        return new_data\n",
    "\n",
    "    # Flip the first row and the leftmost column\n",
    "    def flip_first_row_and_leftmost_col(data):\n",
    "        \"\"\"\n",
    "        Flips the first row and the leftmost column in a 2D list.\n",
    "        Returns a new 2D list with the flipped values.\n",
    "        \"\"\"\n",
    "        if not data or not data[0]:\n",
    "            return data\n",
    "        num_rows = len(data)\n",
    "        num_cols = len(data[0])\n",
    "\n",
    "        # Extract first row and leftmost column\n",
    "        first_row = data[0]\n",
    "        leftmost_col = [data[i][0] for i in range(num_rows)]\n",
    "\n",
    "        # Swap the values\n",
    "        new_data = [[data[i][j] for j in range(num_cols)] for i in range(num_rows)]\n",
    "        new_data[0] = leftmost_col\n",
    "        for i in range(num_rows):\n",
    "            new_data[i][0] = first_row[i] if i < len(first_row) else None  # Handle any length mismatch\n",
    "\n",
    "        return new_data\n",
    "\n",
    "    # Transpose the 2D list\n",
    "    def transpose(data):\n",
    "        \"\"\"\n",
    "        Transposes a 2D list, flipping rows and columns.\n",
    "        \"\"\"\n",
    "        return [list(row) for row in zip(*data)]\n",
    "\n",
    "    # Rotate the 2D list 90 degrees clockwise\n",
    "    def rotate_90_clockwise(data):\n",
    "        \"\"\"\n",
    "        Rotates the 2D list 90 degrees clockwise.\n",
    "        \"\"\"\n",
    "        return [list(row) for row in zip(*data[::-1])]\n",
    "\n",
    "    # Swap the first two columns\n",
    "    def swap_first_two_columns(data):\n",
    "        \"\"\"\n",
    "        Swaps the first two columns in a 2D list.\n",
    "        \"\"\"\n",
    "        if len(data[0]) < 2:\n",
    "            raise ValueError(\"Not enough columns to swap.\")\n",
    "        return [[row[1], row[0]] + row[2:] for row in data]\n",
    "\n",
    "    # Add the first two rows element-wise\n",
    "    def add_first_two_rows(data):\n",
    "        \"\"\"\n",
    "        Adds the first two rows element-wise in a 2D list.\n",
    "        Returns a new row and a modified 2D list.\n",
    "        \"\"\"\n",
    "        if len(data) < 2:\n",
    "            raise ValueError(\"Not enough rows to add.\")\n",
    "        new_row = [data[0][i] + data[1][i] for i in range(len(data[0]))]\n",
    "        new_data = [new_row] + data[2:]\n",
    "        return new_data\n",
    "\n",
    "    # Remove duplicate elements in each row of the 2D list\n",
    "    def remove_duplicates(data):\n",
    "        \"\"\"\n",
    "        Removes duplicate elements in each row of a 2D list.\n",
    "        Returns a new 2D list with duplicates removed, preserving the original order.\n",
    "        \"\"\"\n",
    "        return [list(dict.fromkeys(row)) for row in data]\n",
    "\n",
    "\n",
    "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
    "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
    "    \n",
    "    input dataset: {input_list}\n",
    "    output dataset: {output_list}\n",
    "    test set: {test_list}\n",
    "    '''\n",
    "### Add operations to the prompt to make it more specific\n",
    "### run foofah base on fixed prompt\n",
    "### LLM generate code -> ask LLM to identify the error -> regenerate the code with the error fixed ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "   api_key='17b5615f064c4a899a308979d2f195e7',\n",
    "   api_version=\"2024-02-15-preview\",\n",
    "   azure_endpoint=\"https://wed-aiq-aoai-swc.openai.azure.com/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.Client(\n",
    "    api_key='sk-v2jfjIB3eI39LpLh4d060e314f534d4f85C74169E7164c76',\n",
    "    base_url = \"https://open.xiaojingai.com/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_data(file_name):\n",
    "    test_data = None\n",
    "    input_data = [''] * 2\n",
    "    output_data = [''] * 2\n",
    "    with open(file_name, 'rb') as f:\n",
    "        test_data = json.load(f)\n",
    "            \n",
    "    input_data[0] = test_data['InputTable']\n",
    "    input_data[1] = test_data['OutputTable']\n",
    "    output_data[0] = test_data['TestingTable']\n",
    "    output_data[1] = test_data['TestAnswer']\n",
    "\n",
    "\n",
    "    return input_data, output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ouput(content):\n",
    "    settings = {\n",
    "            \"model\": \"gpt-4o\",\n",
    "            \"temperature\": 0,\n",
    "            \"seed\": 1,\n",
    "        }\n",
    "    answer = []\n",
    "    message = [\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        messages=message, stream=False, **settings\n",
    "    )\n",
    "    if response.choices:\n",
    "        client_response = response.choices[0].message.content\n",
    "        answer.append(client_response)\n",
    "    return client_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'foofah_kg.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def gpt_output():\n",
    "        result= []\n",
    "        #rerun = [47,37,29,26,6,'crime_data_wrangler','potters_wheel_unfold','proactive_wrangling_fold']\n",
    "        l = [2, 7, 17, 19, 22,26,29, 30,33,34,37,40,43,44,46,47,'potters_wheel_unfold','proactive_wrangling_complex']\n",
    "        time = [6,12,51]\n",
    "        name_list = [\"craigslist_data_wrangler\", \"crime_data_wrangler\", \"potters_wheel_divide\", \"potters_wheel_fold\" ,\n",
    "                     \"potters_wheel_fold_2\", \"potters_wheel_merge_split\", \"potters_wheel_split_fold\", \"potters_wheel_unfold\", \n",
    "                     \"potters_wheel_unfold2\", \"proactive_wrangling_fold\", \"proactive_wrangling_complex\", \"reshape_table_structure_data_wrangler\"]\n",
    "        for j in l:\n",
    "            for p in range(1):\n",
    "                for i in range(1,6):\n",
    "                    path = '../../data/foofah/foofah/exp0_'+str(j) + '_'+ str(i)+ '.txt'\n",
    "                    #path = '../../data/foofah/Transformation.Text/Language.00000'+str(i)+'.txt'\n",
    "                    input_data, test_data = read_in_data(path)\n",
    "                    print('on file', j,i)\n",
    "                    prompt = TEMPLATE_WITH_FUNC.format(\n",
    "                        input_list = input_data[0],\n",
    "                        output_list = input_data[1],\n",
    "                        test_list = test_data[0]\n",
    "                    )\n",
    "                    answer = get_ouput(prompt)\n",
    "                    \n",
    "                    # code = answer.split('```python')[1]\n",
    "                    # code = code.split('```')[0]\n",
    "                        \n",
    "                    result.append({'foofah_kg'+str(j)+'_'+ str(1): answer})\n",
    "                    with open(output_path, \"a\") as f:\n",
    "                        json.dump({'foofah_kg'+str(j)+'_'+ str(1): answer}, f)\n",
    "\n",
    "        # path = '../data/foofah/Transformation.Text/Address.000002.json'\n",
    "        # input_data, test_data = read_in_data(path)\n",
    "        # llm = self.environemnt_setup()\n",
    "\n",
    "        # p_tutorial= PromptTemplate(input_variables=['input_list', 'output_list'],\n",
    "        #                             template=template_c)\n",
    "        # chain1 = LLMChain(llm = llm, prompt = p_tutorial)\n",
    "        # with get_openai_callback() as cb:\n",
    "        #     tutorial = chain1.run({'input_list': input_data[0], 'output_list': input_data[1]})\n",
    "        #     print(cb.total_tokens)\n",
    "                \n",
    "        # result.append(tutorial)\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, \"r\") as f:\n",
    "    data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code(code):\n",
    "    output = io.StringIO()\n",
    "\n",
    "    # Use contextlib.redirect_stdout to redirect print statements to the StringIO object\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(output):\n",
    "            exec(code)\n",
    "\n",
    "        # Get the output as a string\n",
    "        captured_output = output.getvalue()\n",
    "    except:\n",
    "        captured_output = []\n",
    "    return captured_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "code = \"\\ndef transform_data(input_list):\\n    \\\"\\\"\\\"\\n    Transforms the input dataset into the required output dataset by concatenating elements from consecutive rows.\\n    \\\"\\\"\\\"\\n    # Initialize an empty list to store the transformed data\\n    output_list = []\\n\\n    # Process every pair of rows in the input list\\n    for i in range(0, len(input_list), 2):\\n        # Concatenate relevant parts of the two consecutive rows\\n        combined_row = input_list[i][:3] + input_list[i+1][1:5]  # Adjusted to [1:5] instead of [1:6]\\n        # Append the combined row to the output list\\n        output_list.append(combined_row)\\n    \\n    return output_list\\n\\n# Define the input list\\ninput_list = [['3099', '905', ' AUST 4WD CUST ACT', '', '', '', ''], ['NO.14', 'NO.14', 'Full Copies', '6.7839', '2', '* *', '0'], ['3200', '906', 'AUST HOUSE & GARDEN', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '4.7385', '1', '* *', '0'], ['3167', '906', 'AUST PERSONAL COMPUTER', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '6.7839', '3', '* *', '0'], ['2929', '924', 'AUTO ACTION', '', '', '', ''], ['#1344', '#1344', 'Covers Only', '3.8181', '3', '* *', '0'], ['5356', '901', 'BEAUT BEADED BAGS', '', '', '', ''], ['# 1', '# 1', 'Full Copies', '10.1929', '3', '* *', '0']]\\n\\n# Transform the input list\\noutput_list = transform_data(input_list)\\n\\n# Print the transformed output list\\nprint(output_list)\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_code' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m run_code(code)\n\u001b[1;32m      2\u001b[0m i, t \u001b[38;5;241m=\u001b[39m read_in_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/foofah/foofah/exp0_29_3.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(i[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_code' is not defined"
     ]
    }
   ],
   "source": [
    "x = run_code(code)\n",
    "i, t = read_in_data('../../data/foofah/foofah/exp0_29_3.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2'],\n",
       " ['3200', '906', 'AUST HOUSE & GARDEN', '9-Jun', 'Covers Only', '4.7385', '1'],\n",
       " ['3167',\n",
       "  '906',\n",
       "  'AUST PERSONAL COMPUTER',\n",
       "  '9-Jun',\n",
       "  'Covers Only',\n",
       "  '6.7839',\n",
       "  '3'],\n",
       " ['2929', '924', 'AUTO ACTION', '#1344', 'Covers Only', '3.8181', '3'],\n",
       " ['5356', '901', 'BEAUT BEADED BAGS', '# 1', 'Full Copies', '10.1929', '3']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "pre = eval(x)\n",
    "print(pre == i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "j_idx = 0\n",
    "i_idx = 1\n",
    "l = [2, 7, 17, 19, 22,26,29, 30,33,34,37,40,43,44,46,47,'potters_wheel_unfold','proactive_wrangling_complex']\n",
    "for d in data:\n",
    "    for key, value in d.items():\n",
    "        print(key)\n",
    "        dataset = key[:-2]\n",
    "        # path = '../../data/foofah/foofah/date_exp0_'+dataset+ '.txt'\n",
    "        # input_data, test_data = read_in_data(path)\n",
    "        # input_data = test_data[0]\n",
    "        # input_dataset = test_data[0]\n",
    "        if i_idx == 6:\n",
    "            i_idx = 1\n",
    "            j_idx += 1\n",
    "        k = str(l[j_idx])+ '_'+ str(i_idx)\n",
    "        code = value.split('```python')[1]\n",
    "        code = code.split('```')[0]\n",
    "        value = run_code(code)\n",
    "        test.append({k: value})\n",
    "        i_idx += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC</td>\n",
       "      <td>766,469</td>\n",
       "      <td>74</td>\n",
       "      <td>703,255</td>\n",
       "      <td>70</td>\n",
       "      <td>631,646</td>\n",
       "      <td>59</td>\n",
       "      <td>2,101,370</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEF</td>\n",
       "      <td>776,996</td>\n",
       "      <td>60</td>\n",
       "      <td>1,532,159</td>\n",
       "      <td>76</td>\n",
       "      <td>494,919</td>\n",
       "      <td>42</td>\n",
       "      <td>2,804,074</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XYZ</td>\n",
       "      <td>832,414</td>\n",
       "      <td>67</td>\n",
       "      <td>897,949</td>\n",
       "      <td>63</td>\n",
       "      <td>712,365</td>\n",
       "      <td>52</td>\n",
       "      <td>2,442,728</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1   2          3   4        5   6          7    8\n",
       "0  ABC  766,469  74    703,255  70  631,646  59  2,101,370  203\n",
       "1  DEF  776,996  60  1,532,159  76  494,919  42  2,804,074  178\n",
       "2  XYZ  832,414  67    897,949  63  712,365  52  2,442,728  182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data, test_data = read_in_data('../../data/foofah/foofah/exp0_29_3.txt')\n",
    "df = pd.DataFrame(input_data[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEF</td>\n",
       "      <td>776,996</td>\n",
       "      <td>60</td>\n",
       "      <td>1,532,159</td>\n",
       "      <td>76</td>\n",
       "      <td>494,919</td>\n",
       "      <td>42</td>\n",
       "      <td>2,804,074</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FYI</td>\n",
       "      <td>818,331</td>\n",
       "      <td>79</td>\n",
       "      <td>349,399</td>\n",
       "      <td>12</td>\n",
       "      <td>552,587</td>\n",
       "      <td>40</td>\n",
       "      <td>1,720,317</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFK</td>\n",
       "      <td>454,724</td>\n",
       "      <td>71</td>\n",
       "      <td>568,168</td>\n",
       "      <td>20</td>\n",
       "      <td>661,672</td>\n",
       "      <td>62</td>\n",
       "      <td>1,684,564</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1   2          3   4        5   6          7    8\n",
       "0  DEF  776,996  60  1,532,159  76  494,919  42  2,804,074  178\n",
       "1  FYI  818,331  79    349,399  12  552,587  40  1,720,317  131\n",
       "2  AFK  454,724  71    568,168  20  661,672  62  1,684,564  153"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(test_data[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on file 2 1\n",
      "PROMPT: \n",
      "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. \n",
      "    These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. \n",
      "    Here are some common transformations to consider, along with examples where appropriate, some sample reference functions are provided but you are not limited or have to use the provided functions:\n",
      "\n",
      "    # Combine the first two rows into one by concatenation\n",
      "    def combine_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Combines the first two rows in a 2D list by concatenating them.\n",
      "        Returns a new 2D list with the combined row.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to combine.\")\n",
      "        new_row = data[0] + data[1]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Flip the first row and the leftmost column\n",
      "    def flip_first_row_and_leftmost_col(data):\n",
      "        \"\"\"\n",
      "        Flips the first row and the leftmost column in a 2D list.\n",
      "        Returns a new 2D list with the flipped values.\n",
      "        \"\"\"\n",
      "        if not data or not data[0]:\n",
      "            return data\n",
      "        num_rows = len(data)\n",
      "        num_cols = len(data[0])\n",
      "\n",
      "        # Extract first row and leftmost column\n",
      "        first_row = data[0]\n",
      "        leftmost_col = [data[i][0] for i in range(num_rows)]\n",
      "\n",
      "        # Swap the values\n",
      "        new_data = [[data[i][j] for j in range(num_cols)] for i in range(num_rows)]\n",
      "        new_data[0] = leftmost_col\n",
      "        for i in range(num_rows):\n",
      "            new_data[i][0] = first_row[i] if i < len(first_row) else None  # Handle any length mismatch\n",
      "\n",
      "        return new_data\n",
      "\n",
      "    # Transpose the 2D list\n",
      "    def transpose(data):\n",
      "        \"\"\"\n",
      "        Transposes a 2D list, flipping rows and columns.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data)]\n",
      "\n",
      "    # Rotate the 2D list 90 degrees clockwise\n",
      "    def rotate_90_clockwise(data):\n",
      "        \"\"\"\n",
      "        Rotates the 2D list 90 degrees clockwise.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data[::-1])]\n",
      "\n",
      "    # Swap the first two columns\n",
      "    def swap_first_two_columns(data):\n",
      "        \"\"\"\n",
      "        Swaps the first two columns in a 2D list.\n",
      "        \"\"\"\n",
      "        if len(data[0]) < 2:\n",
      "            raise ValueError(\"Not enough columns to swap.\")\n",
      "        return [[row[1], row[0]] + row[2:] for row in data]\n",
      "\n",
      "    # Add the first two rows element-wise\n",
      "    def add_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Adds the first two rows element-wise in a 2D list.\n",
      "        Returns a new row and a modified 2D list.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to add.\")\n",
      "        new_row = [data[0][i] + data[1][i] for i in range(len(data[0]))]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Remove duplicate elements in each row of the 2D list\n",
      "    def remove_duplicates(data):\n",
      "        \"\"\"\n",
      "        Removes duplicate elements in each row of a 2D list.\n",
      "        Returns a new 2D list with duplicates removed, preserving the original order.\n",
      "        \"\"\"\n",
      "        return [list(dict.fromkeys(row)) for row in data]\n",
      "\n",
      "\n",
      "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
      "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
      "    \n",
      "    input dataset: [['3099', '905', ' AUST 4WD CUST ACT', '', '', '', ''], ['NO.14', 'NO.14', 'Full Copies', '6.7839', '2', '* *', '0']]\n",
      "    output dataset: [['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2']]\n",
      "    test set: [['3200', '906', 'AUST HOUSE & GARDEN', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '4.7385', '1', '* *', '0'], ['3167', '906', 'AUST PERSONAL COMPUTER', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '6.7839', '3', '* *', '0'], ['2929', '924', 'AUTO ACTION', '', '', '', ''], ['#1344', '#1344', 'Covers Only', '3.8181', '3', '* *', '0'], ['5356', '901', 'BEAUT BEADED BAGS', '', '', '', ''], ['# 1', '# 1', 'Full Copies', '10.1929', '3', '* *', '0'], ['5950', '901', 'BETTER PHOTOGRAPHY', '', '', '', ''], ['#55', '#55', 'Full Copies', '8.8293', '3', '* *', '0'], ['60009', '905', 'BLISS', '', '', '', ''], ['9-Apr', '9-Apr', 'Covers Only', '4.0567', '1', '* *', '0'], ['6454', '911', 'CARRY ON DVD NAT', '', '', '', ''], ['NO 32', 'NO 32', 'Full Copies', '13.6019', '1', '* *', '0'], ['7127', '911', 'CL WAR MOVIES NAT', '', '', '', ''], ['NO 30', 'NO 30', 'Full Copies', '13.6019', '1', '* *', '0'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', '', '', '', ''], ['WINT09', 'WINT09', 'Full Copies', '9.2043', '3', '* *', '0'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', '', '', '', ''], ['V14#11', 'V14#11', 'Full Copies', '6.7839', '3', '* *', '0'], ['7663', '911', 'DORA DOLL NAT', '', '', '', ''], ['NO 24', 'NO 24', 'Full Copies', '5.4203', '1', '* *', '0'], ['1876', '924', 'ECONOMIST THE', '', '', '', ''], ['13-Jun', '13-Jun', 'Covers Only', '7.1589', '4', '* *', '0'], ['1942', '923', 'ENG WOMANS WEEKLY', '', '', '', ''], ['16-Jun', '16-Jun', 'Covers Only', '2.2499', '1', '* *', '0'], ['5919', '906', 'FAST FOURS', '', '', '', ''], ['9-Jun', '9-Jun', 'Full Copies', '6.7839', '2', '* *', '0']]\n",
      "    \n",
      "on file 2 2\n",
      "PROMPT: \n",
      "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. \n",
      "    These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. \n",
      "    Here are some common transformations to consider, along with examples where appropriate, some sample reference functions are provided but you are not limited or have to use the provided functions:\n",
      "\n",
      "    # Combine the first two rows into one by concatenation\n",
      "    def combine_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Combines the first two rows in a 2D list by concatenating them.\n",
      "        Returns a new 2D list with the combined row.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to combine.\")\n",
      "        new_row = data[0] + data[1]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Flip the first row and the leftmost column\n",
      "    def flip_first_row_and_leftmost_col(data):\n",
      "        \"\"\"\n",
      "        Flips the first row and the leftmost column in a 2D list.\n",
      "        Returns a new 2D list with the flipped values.\n",
      "        \"\"\"\n",
      "        if not data or not data[0]:\n",
      "            return data\n",
      "        num_rows = len(data)\n",
      "        num_cols = len(data[0])\n",
      "\n",
      "        # Extract first row and leftmost column\n",
      "        first_row = data[0]\n",
      "        leftmost_col = [data[i][0] for i in range(num_rows)]\n",
      "\n",
      "        # Swap the values\n",
      "        new_data = [[data[i][j] for j in range(num_cols)] for i in range(num_rows)]\n",
      "        new_data[0] = leftmost_col\n",
      "        for i in range(num_rows):\n",
      "            new_data[i][0] = first_row[i] if i < len(first_row) else None  # Handle any length mismatch\n",
      "\n",
      "        return new_data\n",
      "\n",
      "    # Transpose the 2D list\n",
      "    def transpose(data):\n",
      "        \"\"\"\n",
      "        Transposes a 2D list, flipping rows and columns.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data)]\n",
      "\n",
      "    # Rotate the 2D list 90 degrees clockwise\n",
      "    def rotate_90_clockwise(data):\n",
      "        \"\"\"\n",
      "        Rotates the 2D list 90 degrees clockwise.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data[::-1])]\n",
      "\n",
      "    # Swap the first two columns\n",
      "    def swap_first_two_columns(data):\n",
      "        \"\"\"\n",
      "        Swaps the first two columns in a 2D list.\n",
      "        \"\"\"\n",
      "        if len(data[0]) < 2:\n",
      "            raise ValueError(\"Not enough columns to swap.\")\n",
      "        return [[row[1], row[0]] + row[2:] for row in data]\n",
      "\n",
      "    # Add the first two rows element-wise\n",
      "    def add_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Adds the first two rows element-wise in a 2D list.\n",
      "        Returns a new row and a modified 2D list.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to add.\")\n",
      "        new_row = [data[0][i] + data[1][i] for i in range(len(data[0]))]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Remove duplicate elements in each row of the 2D list\n",
      "    def remove_duplicates(data):\n",
      "        \"\"\"\n",
      "        Removes duplicate elements in each row of a 2D list.\n",
      "        Returns a new 2D list with duplicates removed, preserving the original order.\n",
      "        \"\"\"\n",
      "        return [list(dict.fromkeys(row)) for row in data]\n",
      "\n",
      "\n",
      "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
      "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
      "    \n",
      "    input dataset: [['3099', '905', ' AUST 4WD CUST ACT', '', '', '', ''], ['NO.14', 'NO.14', 'Full Copies', '6.7839', '2', '* *', '0'], ['3200', '906', 'AUST HOUSE & GARDEN', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '4.7385', '1', '* *', '0']]\n",
      "    output dataset: [['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2'], ['3200', '906', 'AUST HOUSE & GARDEN', '9-Jun', 'Covers Only', '4.7385', '1']]\n",
      "    test set: [['3167', '906', 'AUST PERSONAL COMPUTER', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '6.7839', '3', '* *', '0'], ['2929', '924', 'AUTO ACTION', '', '', '', ''], ['#1344', '#1344', 'Covers Only', '3.8181', '3', '* *', '0'], ['5356', '901', 'BEAUT BEADED BAGS', '', '', '', ''], ['# 1', '# 1', 'Full Copies', '10.1929', '3', '* *', '0'], ['5950', '901', 'BETTER PHOTOGRAPHY', '', '', '', ''], ['#55', '#55', 'Full Copies', '8.8293', '3', '* *', '0'], ['60009', '905', 'BLISS', '', '', '', ''], ['9-Apr', '9-Apr', 'Covers Only', '4.0567', '1', '* *', '0'], ['6454', '911', 'CARRY ON DVD NAT', '', '', '', ''], ['NO 32', 'NO 32', 'Full Copies', '13.6019', '1', '* *', '0'], ['7127', '911', 'CL WAR MOVIES NAT', '', '', '', ''], ['NO 30', 'NO 30', 'Full Copies', '13.6019', '1', '* *', '0'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', '', '', '', ''], ['WINT09', 'WINT09', 'Full Copies', '9.2043', '3', '* *', '0'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', '', '', '', ''], ['V14#11', 'V14#11', 'Full Copies', '6.7839', '3', '* *', '0'], ['7663', '911', 'DORA DOLL NAT', '', '', '', ''], ['NO 24', 'NO 24', 'Full Copies', '5.4203', '1', '* *', '0'], ['1876', '924', 'ECONOMIST THE', '', '', '', ''], ['13-Jun', '13-Jun', 'Covers Only', '7.1589', '4', '* *', '0'], ['1942', '923', 'ENG WOMANS WEEKLY', '', '', '', ''], ['16-Jun', '16-Jun', 'Covers Only', '2.2499', '1', '* *', '0'], ['5919', '906', 'FAST FOURS', '', '', '', ''], ['9-Jun', '9-Jun', 'Full Copies', '6.7839', '2', '* *', '0']]\n",
      "    \n",
      "on file 2 3\n",
      "PROMPT: \n",
      "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. \n",
      "    These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. \n",
      "    Here are some common transformations to consider, along with examples where appropriate, some sample reference functions are provided but you are not limited or have to use the provided functions:\n",
      "\n",
      "    # Combine the first two rows into one by concatenation\n",
      "    def combine_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Combines the first two rows in a 2D list by concatenating them.\n",
      "        Returns a new 2D list with the combined row.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to combine.\")\n",
      "        new_row = data[0] + data[1]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Flip the first row and the leftmost column\n",
      "    def flip_first_row_and_leftmost_col(data):\n",
      "        \"\"\"\n",
      "        Flips the first row and the leftmost column in a 2D list.\n",
      "        Returns a new 2D list with the flipped values.\n",
      "        \"\"\"\n",
      "        if not data or not data[0]:\n",
      "            return data\n",
      "        num_rows = len(data)\n",
      "        num_cols = len(data[0])\n",
      "\n",
      "        # Extract first row and leftmost column\n",
      "        first_row = data[0]\n",
      "        leftmost_col = [data[i][0] for i in range(num_rows)]\n",
      "\n",
      "        # Swap the values\n",
      "        new_data = [[data[i][j] for j in range(num_cols)] for i in range(num_rows)]\n",
      "        new_data[0] = leftmost_col\n",
      "        for i in range(num_rows):\n",
      "            new_data[i][0] = first_row[i] if i < len(first_row) else None  # Handle any length mismatch\n",
      "\n",
      "        return new_data\n",
      "\n",
      "    # Transpose the 2D list\n",
      "    def transpose(data):\n",
      "        \"\"\"\n",
      "        Transposes a 2D list, flipping rows and columns.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data)]\n",
      "\n",
      "    # Rotate the 2D list 90 degrees clockwise\n",
      "    def rotate_90_clockwise(data):\n",
      "        \"\"\"\n",
      "        Rotates the 2D list 90 degrees clockwise.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data[::-1])]\n",
      "\n",
      "    # Swap the first two columns\n",
      "    def swap_first_two_columns(data):\n",
      "        \"\"\"\n",
      "        Swaps the first two columns in a 2D list.\n",
      "        \"\"\"\n",
      "        if len(data[0]) < 2:\n",
      "            raise ValueError(\"Not enough columns to swap.\")\n",
      "        return [[row[1], row[0]] + row[2:] for row in data]\n",
      "\n",
      "    # Add the first two rows element-wise\n",
      "    def add_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Adds the first two rows element-wise in a 2D list.\n",
      "        Returns a new row and a modified 2D list.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to add.\")\n",
      "        new_row = [data[0][i] + data[1][i] for i in range(len(data[0]))]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Remove duplicate elements in each row of the 2D list\n",
      "    def remove_duplicates(data):\n",
      "        \"\"\"\n",
      "        Removes duplicate elements in each row of a 2D list.\n",
      "        Returns a new 2D list with duplicates removed, preserving the original order.\n",
      "        \"\"\"\n",
      "        return [list(dict.fromkeys(row)) for row in data]\n",
      "\n",
      "\n",
      "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
      "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
      "    \n",
      "    input dataset: [['3099', '905', ' AUST 4WD CUST ACT', '', '', '', ''], ['NO.14', 'NO.14', 'Full Copies', '6.7839', '2', '* *', '0'], ['3200', '906', 'AUST HOUSE & GARDEN', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '4.7385', '1', '* *', '0'], ['3167', '906', 'AUST PERSONAL COMPUTER', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '6.7839', '3', '* *', '0']]\n",
      "    output dataset: [['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2'], ['3200', '906', 'AUST HOUSE & GARDEN', '9-Jun', 'Covers Only', '4.7385', '1'], ['3167', '906', 'AUST PERSONAL COMPUTER', '9-Jun', 'Covers Only', '6.7839', '3']]\n",
      "    test set: [['2929', '924', 'AUTO ACTION', '', '', '', ''], ['#1344', '#1344', 'Covers Only', '3.8181', '3', '* *', '0'], ['5356', '901', 'BEAUT BEADED BAGS', '', '', '', ''], ['# 1', '# 1', 'Full Copies', '10.1929', '3', '* *', '0'], ['5950', '901', 'BETTER PHOTOGRAPHY', '', '', '', ''], ['#55', '#55', 'Full Copies', '8.8293', '3', '* *', '0'], ['60009', '905', 'BLISS', '', '', '', ''], ['9-Apr', '9-Apr', 'Covers Only', '4.0567', '1', '* *', '0'], ['6454', '911', 'CARRY ON DVD NAT', '', '', '', ''], ['NO 32', 'NO 32', 'Full Copies', '13.6019', '1', '* *', '0'], ['7127', '911', 'CL WAR MOVIES NAT', '', '', '', ''], ['NO 30', 'NO 30', 'Full Copies', '13.6019', '1', '* *', '0'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', '', '', '', ''], ['WINT09', 'WINT09', 'Full Copies', '9.2043', '3', '* *', '0'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', '', '', '', ''], ['V14#11', 'V14#11', 'Full Copies', '6.7839', '3', '* *', '0'], ['7663', '911', 'DORA DOLL NAT', '', '', '', ''], ['NO 24', 'NO 24', 'Full Copies', '5.4203', '1', '* *', '0'], ['1876', '924', 'ECONOMIST THE', '', '', '', ''], ['13-Jun', '13-Jun', 'Covers Only', '7.1589', '4', '* *', '0'], ['1942', '923', 'ENG WOMANS WEEKLY', '', '', '', ''], ['16-Jun', '16-Jun', 'Covers Only', '2.2499', '1', '* *', '0'], ['5919', '906', 'FAST FOURS', '', '', '', ''], ['9-Jun', '9-Jun', 'Full Copies', '6.7839', '2', '* *', '0']]\n",
      "    \n",
      "on file 2 4\n",
      "PROMPT: \n",
      "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. \n",
      "    These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. \n",
      "    Here are some common transformations to consider, along with examples where appropriate, some sample reference functions are provided but you are not limited or have to use the provided functions:\n",
      "\n",
      "    # Combine the first two rows into one by concatenation\n",
      "    def combine_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Combines the first two rows in a 2D list by concatenating them.\n",
      "        Returns a new 2D list with the combined row.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to combine.\")\n",
      "        new_row = data[0] + data[1]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Flip the first row and the leftmost column\n",
      "    def flip_first_row_and_leftmost_col(data):\n",
      "        \"\"\"\n",
      "        Flips the first row and the leftmost column in a 2D list.\n",
      "        Returns a new 2D list with the flipped values.\n",
      "        \"\"\"\n",
      "        if not data or not data[0]:\n",
      "            return data\n",
      "        num_rows = len(data)\n",
      "        num_cols = len(data[0])\n",
      "\n",
      "        # Extract first row and leftmost column\n",
      "        first_row = data[0]\n",
      "        leftmost_col = [data[i][0] for i in range(num_rows)]\n",
      "\n",
      "        # Swap the values\n",
      "        new_data = [[data[i][j] for j in range(num_cols)] for i in range(num_rows)]\n",
      "        new_data[0] = leftmost_col\n",
      "        for i in range(num_rows):\n",
      "            new_data[i][0] = first_row[i] if i < len(first_row) else None  # Handle any length mismatch\n",
      "\n",
      "        return new_data\n",
      "\n",
      "    # Transpose the 2D list\n",
      "    def transpose(data):\n",
      "        \"\"\"\n",
      "        Transposes a 2D list, flipping rows and columns.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data)]\n",
      "\n",
      "    # Rotate the 2D list 90 degrees clockwise\n",
      "    def rotate_90_clockwise(data):\n",
      "        \"\"\"\n",
      "        Rotates the 2D list 90 degrees clockwise.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data[::-1])]\n",
      "\n",
      "    # Swap the first two columns\n",
      "    def swap_first_two_columns(data):\n",
      "        \"\"\"\n",
      "        Swaps the first two columns in a 2D list.\n",
      "        \"\"\"\n",
      "        if len(data[0]) < 2:\n",
      "            raise ValueError(\"Not enough columns to swap.\")\n",
      "        return [[row[1], row[0]] + row[2:] for row in data]\n",
      "\n",
      "    # Add the first two rows element-wise\n",
      "    def add_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Adds the first two rows element-wise in a 2D list.\n",
      "        Returns a new row and a modified 2D list.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to add.\")\n",
      "        new_row = [data[0][i] + data[1][i] for i in range(len(data[0]))]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Remove duplicate elements in each row of the 2D list\n",
      "    def remove_duplicates(data):\n",
      "        \"\"\"\n",
      "        Removes duplicate elements in each row of a 2D list.\n",
      "        Returns a new 2D list with duplicates removed, preserving the original order.\n",
      "        \"\"\"\n",
      "        return [list(dict.fromkeys(row)) for row in data]\n",
      "\n",
      "\n",
      "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
      "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
      "    \n",
      "    input dataset: [['3099', '905', ' AUST 4WD CUST ACT', '', '', '', ''], ['NO.14', 'NO.14', 'Full Copies', '6.7839', '2', '* *', '0'], ['3200', '906', 'AUST HOUSE & GARDEN', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '4.7385', '1', '* *', '0'], ['3167', '906', 'AUST PERSONAL COMPUTER', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '6.7839', '3', '* *', '0'], ['2929', '924', 'AUTO ACTION', '', '', '', ''], ['#1344', '#1344', 'Covers Only', '3.8181', '3', '* *', '0']]\n",
      "    output dataset: [['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2'], ['3200', '906', 'AUST HOUSE & GARDEN', '9-Jun', 'Covers Only', '4.7385', '1'], ['3167', '906', 'AUST PERSONAL COMPUTER', '9-Jun', 'Covers Only', '6.7839', '3'], ['2929', '924', 'AUTO ACTION', '#1344', 'Covers Only', '3.8181', '3']]\n",
      "    test set: [['5356', '901', 'BEAUT BEADED BAGS', '', '', '', ''], ['# 1', '# 1', 'Full Copies', '10.1929', '3', '* *', '0'], ['5950', '901', 'BETTER PHOTOGRAPHY', '', '', '', ''], ['#55', '#55', 'Full Copies', '8.8293', '3', '* *', '0'], ['60009', '905', 'BLISS', '', '', '', ''], ['9-Apr', '9-Apr', 'Covers Only', '4.0567', '1', '* *', '0'], ['6454', '911', 'CARRY ON DVD NAT', '', '', '', ''], ['NO 32', 'NO 32', 'Full Copies', '13.6019', '1', '* *', '0'], ['7127', '911', 'CL WAR MOVIES NAT', '', '', '', ''], ['NO 30', 'NO 30', 'Full Copies', '13.6019', '1', '* *', '0'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', '', '', '', ''], ['WINT09', 'WINT09', 'Full Copies', '9.2043', '3', '* *', '0'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', '', '', '', ''], ['V14#11', 'V14#11', 'Full Copies', '6.7839', '3', '* *', '0'], ['7663', '911', 'DORA DOLL NAT', '', '', '', ''], ['NO 24', 'NO 24', 'Full Copies', '5.4203', '1', '* *', '0'], ['1876', '924', 'ECONOMIST THE', '', '', '', ''], ['13-Jun', '13-Jun', 'Covers Only', '7.1589', '4', '* *', '0'], ['1942', '923', 'ENG WOMANS WEEKLY', '', '', '', ''], ['16-Jun', '16-Jun', 'Covers Only', '2.2499', '1', '* *', '0'], ['5919', '906', 'FAST FOURS', '', '', '', ''], ['9-Jun', '9-Jun', 'Full Copies', '6.7839', '2', '* *', '0']]\n",
      "    \n",
      "on file 2 5\n",
      "PROMPT: \n",
      "    Given an example input and output dataset, learn the transformations applied to convert the input dataset into the output dataset. \n",
      "    These transformations may involve various data cleaning and wrangling tasks that standardize and prepare the data for analysis. \n",
      "    Here are some common transformations to consider, along with examples where appropriate, some sample reference functions are provided but you are not limited or have to use the provided functions:\n",
      "\n",
      "    # Combine the first two rows into one by concatenation\n",
      "    def combine_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Combines the first two rows in a 2D list by concatenating them.\n",
      "        Returns a new 2D list with the combined row.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to combine.\")\n",
      "        new_row = data[0] + data[1]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Flip the first row and the leftmost column\n",
      "    def flip_first_row_and_leftmost_col(data):\n",
      "        \"\"\"\n",
      "        Flips the first row and the leftmost column in a 2D list.\n",
      "        Returns a new 2D list with the flipped values.\n",
      "        \"\"\"\n",
      "        if not data or not data[0]:\n",
      "            return data\n",
      "        num_rows = len(data)\n",
      "        num_cols = len(data[0])\n",
      "\n",
      "        # Extract first row and leftmost column\n",
      "        first_row = data[0]\n",
      "        leftmost_col = [data[i][0] for i in range(num_rows)]\n",
      "\n",
      "        # Swap the values\n",
      "        new_data = [[data[i][j] for j in range(num_cols)] for i in range(num_rows)]\n",
      "        new_data[0] = leftmost_col\n",
      "        for i in range(num_rows):\n",
      "            new_data[i][0] = first_row[i] if i < len(first_row) else None  # Handle any length mismatch\n",
      "\n",
      "        return new_data\n",
      "\n",
      "    # Transpose the 2D list\n",
      "    def transpose(data):\n",
      "        \"\"\"\n",
      "        Transposes a 2D list, flipping rows and columns.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data)]\n",
      "\n",
      "    # Rotate the 2D list 90 degrees clockwise\n",
      "    def rotate_90_clockwise(data):\n",
      "        \"\"\"\n",
      "        Rotates the 2D list 90 degrees clockwise.\n",
      "        \"\"\"\n",
      "        return [list(row) for row in zip(*data[::-1])]\n",
      "\n",
      "    # Swap the first two columns\n",
      "    def swap_first_two_columns(data):\n",
      "        \"\"\"\n",
      "        Swaps the first two columns in a 2D list.\n",
      "        \"\"\"\n",
      "        if len(data[0]) < 2:\n",
      "            raise ValueError(\"Not enough columns to swap.\")\n",
      "        return [[row[1], row[0]] + row[2:] for row in data]\n",
      "\n",
      "    # Add the first two rows element-wise\n",
      "    def add_first_two_rows(data):\n",
      "        \"\"\"\n",
      "        Adds the first two rows element-wise in a 2D list.\n",
      "        Returns a new row and a modified 2D list.\n",
      "        \"\"\"\n",
      "        if len(data) < 2:\n",
      "            raise ValueError(\"Not enough rows to add.\")\n",
      "        new_row = [data[0][i] + data[1][i] for i in range(len(data[0]))]\n",
      "        new_data = [new_row] + data[2:]\n",
      "        return new_data\n",
      "\n",
      "    # Remove duplicate elements in each row of the 2D list\n",
      "    def remove_duplicates(data):\n",
      "        \"\"\"\n",
      "        Removes duplicate elements in each row of a 2D list.\n",
      "        Returns a new 2D list with duplicates removed, preserving the original order.\n",
      "        \"\"\"\n",
      "        return [list(dict.fromkeys(row)) for row in data]\n",
      "\n",
      "\n",
      "    Based on these transformations, generate a Python function that will take a new test set as input and apply the same transformation. \n",
      "    Include test_list as the function parameter and ensure the code only outputs the function definition. Do not include the original input_list or output_list directly in the code.\n",
      "    \n",
      "    input dataset: [['3099', '905', ' AUST 4WD CUST ACT', '', '', '', ''], ['NO.14', 'NO.14', 'Full Copies', '6.7839', '2', '* *', '0'], ['3200', '906', 'AUST HOUSE & GARDEN', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '4.7385', '1', '* *', '0'], ['3167', '906', 'AUST PERSONAL COMPUTER', '', '', '', ''], ['9-Jun', '9-Jun', 'Covers Only', '6.7839', '3', '* *', '0'], ['2929', '924', 'AUTO ACTION', '', '', '', ''], ['#1344', '#1344', 'Covers Only', '3.8181', '3', '* *', '0'], ['5356', '901', 'BEAUT BEADED BAGS', '', '', '', ''], ['# 1', '# 1', 'Full Copies', '10.1929', '3', '* *', '0']]\n",
      "    output dataset: [['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2'], ['3200', '906', 'AUST HOUSE & GARDEN', '9-Jun', 'Covers Only', '4.7385', '1'], ['3167', '906', 'AUST PERSONAL COMPUTER', '9-Jun', 'Covers Only', '6.7839', '3'], ['2929', '924', 'AUTO ACTION', '#1344', 'Covers Only', '3.8181', '3'], ['5356', '901', 'BEAUT BEADED BAGS', '# 1', 'Full Copies', '10.1929', '3']]\n",
      "    test set: [['5950', '901', 'BETTER PHOTOGRAPHY', '', '', '', ''], ['#55', '#55', 'Full Copies', '8.8293', '3', '* *', '0'], ['60009', '905', 'BLISS', '', '', '', ''], ['9-Apr', '9-Apr', 'Covers Only', '4.0567', '1', '* *', '0'], ['6454', '911', 'CARRY ON DVD NAT', '', '', '', ''], ['NO 32', 'NO 32', 'Full Copies', '13.6019', '1', '* *', '0'], ['7127', '911', 'CL WAR MOVIES NAT', '', '', '', ''], ['NO 30', 'NO 30', 'Full Copies', '13.6019', '1', '* *', '0'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', '', '', '', ''], ['WINT09', 'WINT09', 'Full Copies', '9.2043', '3', '* *', '0'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', '', '', '', ''], ['V14#11', 'V14#11', 'Full Copies', '6.7839', '3', '* *', '0'], ['7663', '911', 'DORA DOLL NAT', '', '', '', ''], ['NO 24', 'NO 24', 'Full Copies', '5.4203', '1', '* *', '0'], ['1876', '924', 'ECONOMIST THE', '', '', '', ''], ['13-Jun', '13-Jun', 'Covers Only', '7.1589', '4', '* *', '0'], ['1942', '923', 'ENG WOMANS WEEKLY', '', '', '', ''], ['16-Jun', '16-Jun', 'Covers Only', '2.2499', '1', '* *', '0'], ['5919', '906', 'FAST FOURS', '', '', '', ''], ['9-Jun', '9-Jun', 'Full Copies', '6.7839', '2', '* *', '0']]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "def gpt_output():\n",
    "        result= []\n",
    "        #rerun = [47,37,29,26,6,'crime_data_wrangler','potters_wheel_unfold','proactive_wrangling_fold']\n",
    "        l = [2]\n",
    "        time = [6,12,51]\n",
    "        name_list = [\"craigslist_data_wrangler\", \"crime_data_wrangler\", \"potters_wheel_divide\", \"potters_wheel_fold\" ,\n",
    "                     \"potters_wheel_fold_2\", \"potters_wheel_merge_split\", \"potters_wheel_split_fold\", \"potters_wheel_unfold\", \n",
    "                     \"potters_wheel_unfold2\", \"proactive_wrangling_fold\", \"proactive_wrangling_complex\", \"reshape_table_structure_data_wrangler\"]\n",
    "        for j in l:\n",
    "            for p in range(1):\n",
    "                for i in range(1,6):\n",
    "                    path = '../../data/foofah/foofah/exp0_'+str(j) + '_'+ str(i)+ '.txt'\n",
    "                    #path = '../../data/foofah/Transformation.Text/Language.00000'+str(i)+'.txt'\n",
    "                    input_data, test_data = read_in_data(path)\n",
    "                    print('on file', j,i)\n",
    "                    prompt = TEMPLATE_WITH_FUNC.format(\n",
    "                        input_list = input_data[0],\n",
    "                        output_list = input_data[1],\n",
    "                        test_list = test_data[0]\n",
    "                    )\n",
    "                    print(\"PROMPT:\", prompt)\n",
    "                    # answer = get_ouput(prompt)\n",
    "                    # print(\"ANSWER:\", answer)\n",
    "gpt_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Revised function to handle out-of-range errors and ensure conditions are correctly evaluated\n",
    "def transform_dataset(input_data):\n",
    "    result = []\n",
    "    i = 0\n",
    "    while i < len(input_data) - 1:  # Ensure we don't go out of bounds\n",
    "        # Check if the next row meets the criteria for combining with the current row\n",
    "        next_row = input_data[i + 1]\n",
    "        if (next_row[0].startswith('#') or next_row[0].startswith('NO') or next_row[0][0].isdigit() or next_row[0].count('-') == 1):\n",
    "            transformed_row = [\n",
    "                input_data[i][0],  # Column 1\n",
    "                input_data[i][1],  # Column 2\n",
    "                input_data[i][2],  # Column 3\n",
    "                next_row[0],       # Column 4 from next row\n",
    "                next_row[2],       # Column 5 from next row\n",
    "                next_row[3],       # Column 6 from next row\n",
    "                next_row[4]        # Column 7 from next row\n",
    "            ]\n",
    "            result.append(transformed_row)\n",
    "            i += 2  # Skip the next row as it's already used\n",
    "        else:\n",
    "            i += 1  # Move to the next row if no transformation is applied\n",
    "    return result\n",
    "\n",
    "# Run the revised function on the test data\n",
    "\n",
    "# Run the function on the test data\n",
    "transformed_test_data = transform_dataset(test_data[0])\n",
    "print(transformed_test_data == test_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5950', '901', 'BETTER PHOTOGRAPHY', '#55', 'Full Copies', '8.8293', '3'],\n",
       " ['60009', '905', 'BLISS', '9-Apr', 'Covers Only', '4.0567', '1'],\n",
       " ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'Full Copies', '13.6019', '1'],\n",
       " ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'Full Copies', '13.6019', '1'],\n",
       " ['WINT09', 'WINT09', 'Full Copies', '5714', 'CRDMKNG STMPNG&PPRCRFT', '', ''],\n",
       " ['V14#11', 'V14#11', 'Full Copies', '7663', 'DORA DOLL NAT', '', ''],\n",
       " ['NO 24', 'NO 24', 'Full Copies', '1876', 'ECONOMIST THE', '', ''],\n",
       " ['13-Jun', '13-Jun', 'Covers Only', '1942', 'ENG WOMANS WEEKLY', '', ''],\n",
       " ['16-Jun', '16-Jun', 'Covers Only', '5919', 'FAST FOURS', '', '']]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['5950', '901', 'BETTER PHOTOGRAPHY', '#55', 'Full Copies', '8.8293', '3'],\n",
       " ['60009', '905', 'BLISS', '9-Apr', 'Covers Only', '4.0567', '1'],\n",
       " ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'Full Copies', '13.6019', '1'],\n",
       " ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'Full Copies', '13.6019', '1'],\n",
       " ['5210',\n",
       "  '902',\n",
       "  'COSMOPOLITAN PREGNANCY',\n",
       "  'WINT09',\n",
       "  'Full Copies',\n",
       "  '9.2043',\n",
       "  '3'],\n",
       " ['5714',\n",
       "  '905',\n",
       "  'CRDMKNG STMPNG&PPRCRFT',\n",
       "  'V14#11',\n",
       "  'Full Copies',\n",
       "  '6.7839',\n",
       "  '3'],\n",
       " ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'Full Copies', '5.4203', '1'],\n",
       " ['1876', '924', 'ECONOMIST THE', '13-Jun', 'Covers Only', '7.1589', '4'],\n",
       " ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', 'Covers Only', '2.2499', '1'],\n",
       " ['5919', '906', 'FAST FOURS', '9-Jun', 'Full Copies', '6.7839', '2']]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3099', '905', ' AUST 4WD CUST ACT', 'NO.14', 'Full Copies', '6.7839', '2']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../data/foofah/foofah/exp0_2_5.txt'\n",
    "input_data, test_data = read_in_data(path)\n",
    "x =[['5356', '901', 'BEAUT BEADED BAGS', '# 1', 'Full Copies', '10.1929', '3'],\n",
    " ['5950', '901', 'BETTER PHOTOGRAPHY', '#55', 'Full Copies', '8.8293', '3'],\n",
    " ['60009', '905', 'BLISS', '9-Apr', 'Covers Only', '4.0567', '1'],\n",
    " ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'Full Copies', '13.6019', '1'],\n",
    " ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'Full Copies', '13.6019', '1'],\n",
    " ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'WINT09', 'Full Copies', '9.2043', '3'],\n",
    " ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'V14#11', 'Full Copies', '6.7839', '3'],\n",
    " ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'Full Copies', '5.4203', '1'],\n",
    " ['1876', '924', 'ECONOMIST THE', '13-Jun', 'Covers Only', '7.1589', '4'],\n",
    " ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', 'Covers Only', '2.2499', '1'],\n",
    " ['5919', '906', 'FAST FOURS', '9-Jun', 'Full Copies', '6.7839', '2']]\n",
    "\n",
    "\n",
    "print(transformed_test_data == test_data[1])\n",
    "input_data[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'English', 'French', 'Math']\n",
      "['Anna', '', '78', '43']\n",
      "['Bob', '96', '54', '']\n",
      "['Rob', '87', '92', '']\n",
      "['Tom', '', '85', '90']\n"
     ]
    }
   ],
   "source": [
    "def transform_data(test_set):\n",
    "    # Extract subjects\n",
    "    subjects = list(set(entry[1] for entry in test_set))\n",
    "    subjects.sort()\n",
    "\n",
    "    # Extract names\n",
    "    names = list(set(entry[0] for entry in test_set))\n",
    "    names.sort()\n",
    "\n",
    "    # Initialize the output with headers\n",
    "    output = [[''] + subjects]\n",
    "\n",
    "    # Initialize dictionary for easy lookup\n",
    "    scores_dict = {(name, subject): '' for name in names for subject in subjects}\n",
    "    \n",
    "    # Fill the dictionary with actual scores\n",
    "    for entry in test_set:\n",
    "        name, subject, score = entry\n",
    "        scores_dict[(name, subject)] = score\n",
    "\n",
    "    # Populate the output dataset using the dictionary\n",
    "    for name in names:\n",
    "        output.append([name] + [scores_dict[(name, subject)] for subject in subjects])\n",
    "\n",
    "    return output\n",
    "\n",
    "# Example test execution\n",
    "test_set = [['Anna', 'Math', '43'], ['Anna', 'French', '78'], ['Bob', 'English', '96'], ['Bob', 'French', '54'], ['Tom', 'Math', '90'], ['Tom', 'French', '85'], ['Rob', 'English', '87'], ['Rob', 'French', '92']]\n",
    "result = transform_data(test_set)\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Math</td>\n",
       "      <td>French</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna</td>\n",
       "      <td>43</td>\n",
       "      <td>78</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob</td>\n",
       "      <td></td>\n",
       "      <td>54</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tom</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rob</td>\n",
       "      <td></td>\n",
       "      <td>92</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1       2        3\n",
       "0        Math  French  English\n",
       "1  Anna    43      78         \n",
       "2   Bob            54       96\n",
       "3   Tom    90      85         \n",
       "4   Rob            92       87"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(test_data[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    " 1. in the prompt, give a list of example operations that it wants to prioritze \n",
    "\n",
    " (could also let gpt to generate the informations about the background about PBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('foofah_kg_result.json', \"w\") as f:\n",
    "    json.dump(test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33195</td>\n",
       "      <td>Promotion</td>\n",
       "      <td>2/13/2009</td>\n",
       "      <td>821017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33195</td>\n",
       "      <td>Promotion</td>\n",
       "      <td>3/30/2009</td>\n",
       "      <td>65320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50999</td>\n",
       "      <td>Inc Actvty -  Year end</td>\n",
       "      <td>2/16/2009</td>\n",
       "      <td>64023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50999</td>\n",
       "      <td>Inc Actvty -  Year end</td>\n",
       "      <td>3/2/2009</td>\n",
       "      <td>64519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50999</td>\n",
       "      <td>Inc Actvty -  Year end</td>\n",
       "      <td>3/2/2009</td>\n",
       "      <td>64519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50999</td>\n",
       "      <td>Inc Actvty -  Year end</td>\n",
       "      <td>3/16/2009</td>\n",
       "      <td>164351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                       1          2       3\n",
       "0  33195               Promotion  2/13/2009  821017\n",
       "1  33195               Promotion  3/30/2009   65320\n",
       "2  50999  Inc Actvty -  Year end  2/16/2009   64023\n",
       "3  50999  Inc Actvty -  Year end   3/2/2009   64519\n",
       "4  50999  Inc Actvty -  Year end   3/2/2009   64519\n",
       "5  50999  Inc Actvty -  Year end  3/16/2009  164351"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "path = '../../data/foofah/foofah/exp0_3_2.txt'\n",
    "input_data, test_data = read_in_data(path)\n",
    "df_1 = pd.DataFrame(input_data[1])\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3099</td>\n",
       "      <td>905</td>\n",
       "      <td>AUST 4WD CUST ACT</td>\n",
       "      <td>NO.14</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>6.7839</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3200</td>\n",
       "      <td>906</td>\n",
       "      <td>AUST HOUSE &amp; GARDEN</td>\n",
       "      <td>9-Jun</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>4.7385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3167</td>\n",
       "      <td>906</td>\n",
       "      <td>AUST PERSONAL COMPUTER</td>\n",
       "      <td>9-Jun</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>6.7839</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1                       2      3            4       5  6\n",
       "0  3099  905       AUST 4WD CUST ACT  NO.14  Full Copies  6.7839  2\n",
       "1  3200  906     AUST HOUSE & GARDEN  9-Jun  Covers Only  4.7385  1\n",
       "2  3167  906  AUST PERSONAL COMPUTER  9-Jun  Covers Only  6.7839  3"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(input_data[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2929</td>\n",
       "      <td>924</td>\n",
       "      <td>AUTO ACTION</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#1344</td>\n",
       "      <td>#1344</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>3.8181</td>\n",
       "      <td>3</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5356</td>\n",
       "      <td>901</td>\n",
       "      <td>BEAUT BEADED BAGS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># 1</td>\n",
       "      <td># 1</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>10.1929</td>\n",
       "      <td>3</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5950</td>\n",
       "      <td>901</td>\n",
       "      <td>BETTER PHOTOGRAPHY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#55</td>\n",
       "      <td>#55</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>8.8293</td>\n",
       "      <td>3</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60009</td>\n",
       "      <td>905</td>\n",
       "      <td>BLISS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9-Apr</td>\n",
       "      <td>9-Apr</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>4.0567</td>\n",
       "      <td>1</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6454</td>\n",
       "      <td>911</td>\n",
       "      <td>CARRY ON DVD NAT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NO 32</td>\n",
       "      <td>NO 32</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>13.6019</td>\n",
       "      <td>1</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7127</td>\n",
       "      <td>911</td>\n",
       "      <td>CL WAR MOVIES NAT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NO 30</td>\n",
       "      <td>NO 30</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>13.6019</td>\n",
       "      <td>1</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5210</td>\n",
       "      <td>902</td>\n",
       "      <td>COSMOPOLITAN PREGNANCY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WINT09</td>\n",
       "      <td>WINT09</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>9.2043</td>\n",
       "      <td>3</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5714</td>\n",
       "      <td>905</td>\n",
       "      <td>CRDMKNG STMPNG&amp;PPRCRFT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>V14#11</td>\n",
       "      <td>V14#11</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>6.7839</td>\n",
       "      <td>3</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7663</td>\n",
       "      <td>911</td>\n",
       "      <td>DORA DOLL NAT</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NO 24</td>\n",
       "      <td>NO 24</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>5.4203</td>\n",
       "      <td>1</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1876</td>\n",
       "      <td>924</td>\n",
       "      <td>ECONOMIST THE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13-Jun</td>\n",
       "      <td>13-Jun</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>7.1589</td>\n",
       "      <td>4</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1942</td>\n",
       "      <td>923</td>\n",
       "      <td>ENG WOMANS WEEKLY</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16-Jun</td>\n",
       "      <td>16-Jun</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>2.2499</td>\n",
       "      <td>1</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5919</td>\n",
       "      <td>906</td>\n",
       "      <td>FAST FOURS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9-Jun</td>\n",
       "      <td>9-Jun</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>6.7839</td>\n",
       "      <td>2</td>\n",
       "      <td>* *</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1                       2        3  4    5  6\n",
       "0     2929     924             AUTO ACTION                    \n",
       "1    #1344   #1344             Covers Only   3.8181  3  * *  0\n",
       "2     5356     901       BEAUT BEADED BAGS                    \n",
       "3      # 1     # 1             Full Copies  10.1929  3  * *  0\n",
       "4     5950     901      BETTER PHOTOGRAPHY                    \n",
       "5      #55     #55             Full Copies   8.8293  3  * *  0\n",
       "6    60009     905                   BLISS                    \n",
       "7    9-Apr   9-Apr             Covers Only   4.0567  1  * *  0\n",
       "8     6454     911        CARRY ON DVD NAT                    \n",
       "9    NO 32   NO 32             Full Copies  13.6019  1  * *  0\n",
       "10    7127     911       CL WAR MOVIES NAT                    \n",
       "11   NO 30   NO 30             Full Copies  13.6019  1  * *  0\n",
       "12    5210     902  COSMOPOLITAN PREGNANCY                    \n",
       "13  WINT09  WINT09             Full Copies   9.2043  3  * *  0\n",
       "14    5714     905  CRDMKNG STMPNG&PPRCRFT                    \n",
       "15  V14#11  V14#11             Full Copies   6.7839  3  * *  0\n",
       "16    7663     911           DORA DOLL NAT                    \n",
       "17   NO 24   NO 24             Full Copies   5.4203  1  * *  0\n",
       "18    1876     924           ECONOMIST THE                    \n",
       "19  13-Jun  13-Jun             Covers Only   7.1589  4  * *  0\n",
       "20    1942     923       ENG WOMANS WEEKLY                    \n",
       "21  16-Jun  16-Jun             Covers Only   2.2499  1  * *  0\n",
       "22    5919     906              FAST FOURS                    \n",
       "23   9-Jun   9-Jun             Full Copies   6.7839  2  * *  0"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(test_data[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2929</td>\n",
       "      <td>924</td>\n",
       "      <td>AUTO ACTION</td>\n",
       "      <td>#1344</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>3.8181</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5356</td>\n",
       "      <td>901</td>\n",
       "      <td>BEAUT BEADED BAGS</td>\n",
       "      <td># 1</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>10.1929</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5950</td>\n",
       "      <td>901</td>\n",
       "      <td>BETTER PHOTOGRAPHY</td>\n",
       "      <td>#55</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>8.8293</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60009</td>\n",
       "      <td>905</td>\n",
       "      <td>BLISS</td>\n",
       "      <td>9-Apr</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>4.0567</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6454</td>\n",
       "      <td>911</td>\n",
       "      <td>CARRY ON DVD NAT</td>\n",
       "      <td>NO 32</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>13.6019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7127</td>\n",
       "      <td>911</td>\n",
       "      <td>CL WAR MOVIES NAT</td>\n",
       "      <td>NO 30</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>13.6019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5210</td>\n",
       "      <td>902</td>\n",
       "      <td>COSMOPOLITAN PREGNANCY</td>\n",
       "      <td>WINT09</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>9.2043</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5714</td>\n",
       "      <td>905</td>\n",
       "      <td>CRDMKNG STMPNG&amp;PPRCRFT</td>\n",
       "      <td>V14#11</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>6.7839</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7663</td>\n",
       "      <td>911</td>\n",
       "      <td>DORA DOLL NAT</td>\n",
       "      <td>NO 24</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>5.4203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1876</td>\n",
       "      <td>924</td>\n",
       "      <td>ECONOMIST THE</td>\n",
       "      <td>13-Jun</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>7.1589</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1942</td>\n",
       "      <td>923</td>\n",
       "      <td>ENG WOMANS WEEKLY</td>\n",
       "      <td>16-Jun</td>\n",
       "      <td>Covers Only</td>\n",
       "      <td>2.2499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5919</td>\n",
       "      <td>906</td>\n",
       "      <td>FAST FOURS</td>\n",
       "      <td>9-Jun</td>\n",
       "      <td>Full Copies</td>\n",
       "      <td>6.7839</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1                       2       3            4        5  6\n",
       "0    2929  924             AUTO ACTION   #1344  Covers Only   3.8181  3\n",
       "1    5356  901       BEAUT BEADED BAGS     # 1  Full Copies  10.1929  3\n",
       "2    5950  901      BETTER PHOTOGRAPHY     #55  Full Copies   8.8293  3\n",
       "3   60009  905                   BLISS   9-Apr  Covers Only   4.0567  1\n",
       "4    6454  911        CARRY ON DVD NAT   NO 32  Full Copies  13.6019  1\n",
       "5    7127  911       CL WAR MOVIES NAT   NO 30  Full Copies  13.6019  1\n",
       "6    5210  902  COSMOPOLITAN PREGNANCY  WINT09  Full Copies   9.2043  3\n",
       "7    5714  905  CRDMKNG STMPNG&PPRCRFT  V14#11  Full Copies   6.7839  3\n",
       "8    7663  911           DORA DOLL NAT   NO 24  Full Copies   5.4203  1\n",
       "9    1876  924           ECONOMIST THE  13-Jun  Covers Only   7.1589  4\n",
       "10   1942  923       ENG WOMANS WEEKLY  16-Jun  Covers Only   2.2499  1\n",
       "11   5919  906              FAST FOURS   9-Jun  Full Copies   6.7839  2"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(test_data[1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'TestAnswer': []\n"
     ]
    }
   ],
   "source": [
    "tmp = []\n",
    "ans = []\n",
    "i = 0\n",
    "for d in test_data[0]:\n",
    "    p = d[:6]\n",
    "    for j in range(6, len(d), 4):\n",
    "        tmp  = p.copy()\n",
    "        tmp.append(d[j])\n",
    "        tmp.append(d[j+1])\n",
    "        tmp.append(d[j+2])\n",
    "        tmp.append(d[j+3])\n",
    "        ans.append(tmp)\n",
    "print(\"'TestAnswer':\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def transform_data_corrected(data):\n",
    "    # Step 1: Separate main records and details records\n",
    "    main_records = [data[i] for i in range(0, len(data), 2)]\n",
    "    details_records = [data[i] for i in range(1, len(data), 2)]\n",
    "    \n",
    "    # Step 2: Correctly combine main records and details records by position with specified column selection\n",
    "    transformed_data = [\n",
    "        main[:3] + [detail[0], detail[2], detail[3], detail[4]]\n",
    "        for main, detail in zip(main_records, details_records)\n",
    "    ]\n",
    "    \n",
    "    return transformed_data\n",
    "x = transform_data_corrected(test_data[0])\n",
    "print(x == test_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Anthony, Scott A.', ['Partner'], ['Palo Alto'], ['Mergers & Acquisitions']],\n",
       " ['Baudler, Mark B.', ['Partner'], ['Palo Alto'], ['Mergers & Acquisitions']],\n",
       " ['Berger, David J.',\n",
       "  ['Partner'],\n",
       "  ['San Francisco, Palo Alto'],\n",
       "  ['Mergers & Acquisitions']]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'],\n",
       " ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'],\n",
       " ['Berger, David J.',\n",
       "  'Partner',\n",
       "  'San Francisco, Palo Alto',\n",
       "  'Mergers & Acquisitions']]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct\n"
     ]
    }
   ],
   "source": [
    "input_table = test_data[0]\n",
    "output_table = []\n",
    "\n",
    "# Process each row in the input_table, skipping the header row\n",
    "for row in input_table[1:]:\n",
    "    year = row[0]\n",
    "    catnum = row[1]\n",
    "    comments = row[2]\n",
    "    \n",
    "    # Loop through months (1 to 12) and create separate rows for each month\n",
    "    for i in range(3, 15):\n",
    "        month = input_table[0][i]  # Get the month number from the header row\n",
    "        value = row[i]  # Get the value corresponding to the month\n",
    "        output_table.append([year, catnum, comments, month, value])\n",
    "\n",
    "# Display the OutputTable\n",
    "output_table\n",
    "# output\n",
    "if output_table == test_data[1]:\n",
    "    print('correct')\n",
    "else:\n",
    "    print(len(output_table), len(test_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', 'Anna', 'Davis'],\n",
       " ['', 'Joan', 'Marsh'],\n",
       " ['', 'Daenerys', 'Rhaenys'],\n",
       " ['Stark, Eddard', '', ''],\n",
       " ['', 'Robb', 'Sansa'],\n",
       " ['Lannister, Tywin', '', ''],\n",
       " ['', 'Jaime', 'Cersei'],\n",
       " ['Hoster, Tully', '', ''],\n",
       " ['', 'Lysa', 'Catelyn']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['14:54', '-72'],\n",
       " ['14:54', '-71'],\n",
       " ['14:54', '-90'],\n",
       " ['14:54', '-89'],\n",
       " ['14:54', '-43'],\n",
       " ['14:54', '-76'],\n",
       " ['14:55', '-45'],\n",
       " ['14:55', '-87'],\n",
       " ['14:55', '-98'],\n",
       " ['14:55', '-90'],\n",
       " ['14:55', '-78'],\n",
       " ['14:55', '-76'],\n",
       " ['14:55', '-87'],\n",
       " ['14:55', '-87'],\n",
       " ['14:55', '-98'],\n",
       " ['14:55', '-45'],\n",
       " ['14:55', '-97'],\n",
       " ['14:55', '-45'],\n",
       " ['14:66', '-76'],\n",
       " ['14:66', '-98'],\n",
       " ['14:66', '-45']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('foofah_kg_result.json', \"r\") as f:\n",
    "    result = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_folder = './error/'\n",
    "source_path = '../output/'\n",
    "from collections import defaultdict\n",
    "def calculate_acc():\n",
    "    acc = defaultdict(list)\n",
    "    for d in result:\n",
    "        for key, value in d.items():\n",
    "            print(key)\n",
    "            dataset = key[:-2]\n",
    "            try:\n",
    "                pre = eval(value)\n",
    "            except:\n",
    "                acc[dataset].append([key, 0])\n",
    "                continue\n",
    "            path = '../../data/foofah/foofah/exp0_'+key+ '.txt'\n",
    "            input_data, test_data = read_in_data(path)\n",
    "            # input_path = source_path+model+'foofah/output_'+str(j) + '_'+ str(i)+ '.json'\n",
    "            #     llama70 = read_llm_output_data(input_path)\n",
    "            #     # gpt_3_5 = read_llm_output_data('../output/Llama-2-13b-chat-hf/foofah/output_data_0_'+str(j) + '_'+ str(i)+ '.json')\n",
    "            #     gpt_4_0 = read_llm_output_data('../output/chat_gpt_4.0/foofah/new_output_data_0_'+str(j) + '_'+ str(i)+ '.json')\n",
    "            #     # if os.path.isfile('../output/foofah/foofah/exp0_results_'+str(j) + '_'+ str(i)+ '.txt'):\n",
    "            #     #     foofah = read_output_data('../output/foofah/foofah/exp0_results_'+str(j) + '_'+ str(i)+ '.txt')\n",
    "            #     # else:\n",
    "            #     #     foofah = [[]]\n",
    "\n",
    "                #compare output_data and test_data\n",
    "            print(pre, test_data[1])\n",
    "            acc_temp = 0\n",
    "                        # print(j,i)\n",
    "            ans = test_data[1]\n",
    "\n",
    "            for k in range(min(len(pre),len(ans))):\n",
    "                for m in range(min(len(pre[k]),len(ans[k]))):\n",
    "                    if pre[k][m] == ans[k][m]:\n",
    "                        acc_temp += 1\n",
    "            total_values = sum(len(inner_list) for inner_list in ans)\n",
    "            test_case_acc = acc_temp/total_values\n",
    "                \n",
    "            acc[dataset].append([key, test_case_acc])\n",
    " \n",
    "\n",
    "    return(acc)\n",
    "                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2_1\n",
      "[['3200', '906', 'AUST HOUSE & GARDEN', 'Covers Only', '4.7385', '1'], ['3167', '906', 'AUST PERSONAL COMPUTER', 'Covers Only', '6.7839', '3'], ['2929', '924', 'AUTO ACTION', 'Covers Only', '3.8181', '3'], ['5356', '901', 'BEAUT BEADED BAGS', 'Full Copies', '10.1929', '3'], ['5950', '901', 'BETTER PHOTOGRAPHY', 'Full Copies', '8.8293', '3'], ['60009', '905', 'BLISS', 'Covers Only', '4.0567', '1'], ['6454', '911', 'CARRY ON DVD NAT', 'Full Copies', '13.6019', '1'], ['7127', '911', 'CL WAR MOVIES NAT', 'Full Copies', '13.6019', '1'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'Full Copies', '9.2043', '3'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'Full Copies', '6.7839', '3'], ['7663', '911', 'DORA DOLL NAT', 'Full Copies', '5.4203', '1'], ['1876', '924', 'ECONOMIST THE', 'Covers Only', '7.1589', '4'], ['1942', '923', 'ENG WOMANS WEEKLY', 'Covers Only', '2.2499', '1'], ['5919', '906', 'FAST FOURS', 'Full Copies', '6.7839', '2']] [['3200', '906', 'AUST HOUSE & GARDEN', '9-Jun', 'Covers Only', '4.7385', '1'], ['3167', '906', 'AUST PERSONAL COMPUTER', '9-Jun', 'Covers Only', '6.7839', '3'], ['2929', '924', 'AUTO ACTION', '#1344', 'Covers Only', '3.8181', '3'], ['5356', '901', 'BEAUT BEADED BAGS', '# 1', 'Full Copies', '10.1929', '3'], ['5950', '901', 'BETTER PHOTOGRAPHY', '#55', 'Full Copies', '8.8293', '3'], ['60009', '905', 'BLISS', '9-Apr', 'Covers Only', '4.0567', '1'], ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'Full Copies', '13.6019', '1'], ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'Full Copies', '13.6019', '1'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'WINT09', 'Full Copies', '9.2043', '3'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'V14#11', 'Full Copies', '6.7839', '3'], ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'Full Copies', '5.4203', '1'], ['1876', '924', 'ECONOMIST THE', '13-Jun', 'Covers Only', '7.1589', '4'], ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', 'Covers Only', '2.2499', '1'], ['5919', '906', 'FAST FOURS', '9-Jun', 'Full Copies', '6.7839', '2']]\n",
      "2_2\n",
      "2_3\n",
      "2_4\n",
      "[['5356', '901', 'BEAUT BEADED BAGS', '# 1', '# 1', 'Full Copies', '10.1929', '3'], ['5950', '901', 'BETTER PHOTOGRAPHY', '#55', '#55', 'Full Copies', '8.8293', '3'], ['60009', '905', 'BLISS', '9-Apr', '9-Apr', 'Covers Only', '4.0567', '1'], ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'NO 32', 'Full Copies', '13.6019', '1'], ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'NO 30', 'Full Copies', '13.6019', '1'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'WINT09', 'WINT09', 'Full Copies', '9.2043', '3'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'V14#11', 'V14#11', 'Full Copies', '6.7839', '3'], ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'NO 24', 'Full Copies', '5.4203', '1'], ['1876', '924', 'ECONOMIST THE', '13-Jun', '13-Jun', 'Covers Only', '7.1589', '4'], ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', '16-Jun', 'Covers Only', '2.2499', '1'], ['5919', '906', 'FAST FOURS', '9-Jun', '9-Jun', 'Full Copies', '6.7839', '2']] [['5356', '901', 'BEAUT BEADED BAGS', '# 1', 'Full Copies', '10.1929', '3'], ['5950', '901', 'BETTER PHOTOGRAPHY', '#55', 'Full Copies', '8.8293', '3'], ['60009', '905', 'BLISS', '9-Apr', 'Covers Only', '4.0567', '1'], ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'Full Copies', '13.6019', '1'], ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'Full Copies', '13.6019', '1'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'WINT09', 'Full Copies', '9.2043', '3'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'V14#11', 'Full Copies', '6.7839', '3'], ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'Full Copies', '5.4203', '1'], ['1876', '924', 'ECONOMIST THE', '13-Jun', 'Covers Only', '7.1589', '4'], ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', 'Covers Only', '2.2499', '1'], ['5919', '906', 'FAST FOURS', '9-Jun', 'Full Copies', '6.7839', '2']]\n",
      "2_5\n",
      "[['5950', '901', 'BETTER PHOTOGRAPHY', '#55', '#55', 'Full Copies', '8.8293', '3'], ['60009', '905', 'BLISS', '9-Apr', '9-Apr', 'Covers Only', '4.0567', '1'], ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'NO 32', 'Full Copies', '13.6019', '1'], ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'NO 30', 'Full Copies', '13.6019', '1'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'WINT09', 'WINT09', 'Full Copies', '9.2043', '3'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'V14#11', 'V14#11', 'Full Copies', '6.7839', '3'], ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'NO 24', 'Full Copies', '5.4203', '1'], ['1876', '924', 'ECONOMIST THE', '13-Jun', '13-Jun', 'Covers Only', '7.1589', '4'], ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', '16-Jun', 'Covers Only', '2.2499', '1'], ['5919', '906', 'FAST FOURS', '9-Jun', '9-Jun', 'Full Copies', '6.7839', '2']] [['5950', '901', 'BETTER PHOTOGRAPHY', '#55', 'Full Copies', '8.8293', '3'], ['60009', '905', 'BLISS', '9-Apr', 'Covers Only', '4.0567', '1'], ['6454', '911', 'CARRY ON DVD NAT', 'NO 32', 'Full Copies', '13.6019', '1'], ['7127', '911', 'CL WAR MOVIES NAT', 'NO 30', 'Full Copies', '13.6019', '1'], ['5210', '902', 'COSMOPOLITAN PREGNANCY', 'WINT09', 'Full Copies', '9.2043', '3'], ['5714', '905', 'CRDMKNG STMPNG&PPRCRFT', 'V14#11', 'Full Copies', '6.7839', '3'], ['7663', '911', 'DORA DOLL NAT', 'NO 24', 'Full Copies', '5.4203', '1'], ['1876', '924', 'ECONOMIST THE', '13-Jun', 'Covers Only', '7.1589', '4'], ['1942', '923', 'ENG WOMANS WEEKLY', '16-Jun', 'Covers Only', '2.2499', '1'], ['5919', '906', 'FAST FOURS', '9-Jun', 'Full Copies', '6.7839', '2']]\n",
      "7_1\n",
      "7_2\n",
      "[['3620', 'Lepper MARY', 'MOLLY', '12/13/2011', '2/14/2011', '11746 N lifty LN', 'SPAYED FEMALE', '$8.00', '2/14/2014', 'MEQUON, WI 53092', 'YORKSHIRE TERRIER', '$0.00', 'N/A', '234234', 'TAN / SILVER', '1020', '44567'], ['3621', 'Koothrapali, Rajesh', 'Cinammon', '12/14/2011', '2/7/2011', '12245 N lifty LN', 'SPAYED FEMALE', '$8.00', '6/17/2014', 'MEQUON, WI 53092', 'YORKSHIRE TERRIER', '$0.00', 'N/A', '234234', 'TAN / SILVER', 'CASH', '25677'], ['3622', 'Cooper, SHELDON', 'Pistol', '12/22/2011', '5/11/2011', '12563 N lifty LN', 'SPAYED FEMALE', '$8.00', '1/8/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678'], ['3622', 'Hofstader, LEONARD', 'Button', '12/8/2011', '5/25/2011', '12812 N lifty LN', 'SPAYED FEMALE', '$8.00', '4/9/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678']] [['3620', 'Lepper MARY', 'MOLLY', '12/13/2011', '2/14/2011', '11746 N lifty LN', 'SPAYED FEMALE', '$8.00', '2/14/2014', 'MEQUON, WI 53092', 'YORKSHIRE TERRIER', '$0.00', 'N/A', '234234', 'TAN / SILVER', '1020', '44567'], ['3621', 'Koothrapali, Rajesh', 'Cinammon', '12/14/2011', '2/7/2011', '12245 N lifty LN', 'SPAYED FEMALE', '$8.00', '6/17/2014', 'MEQUON, WI 53092', 'YORKSHIRE TERRIER', '$0.00', 'N/A', '234234', 'TAN / SILVER', 'CASH', '25677'], ['3622', 'Cooper, SHELDON', 'Pistol', '12/22/2011', '5/11/2011', '12563 N lifty LN', 'SPAYED FEMALE', '$8.00', '1/8/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678'], ['3622', 'Hofstader, LEONARD', 'Button', '12/8/2011', '5/25/2011', '12812 N lifty LN', 'SPAYED FEMALE', '$8.00', '4/9/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678']]\n",
      "7_3\n",
      "7_4\n",
      "[['3622', 'Cooper, SHELDON', 'Pistol', '12/22/2011', '5/11/2011', '12563 N lifty LN', 'SPAYED FEMALE', '$8.00', '1/8/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678'], ['3622', 'Hofstader, LEONARD', 'Button', '12/8/2011', '5/25/2011', '12812 N lifty LN', 'SPAYED FEMALE', '$8.00', '4/9/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678']] [['3622', 'Cooper, SHELDON', 'Pistol', '12/22/2011', '5/11/2011', '12563 N lifty LN', 'SPAYED FEMALE', '$8.00', '1/8/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678'], ['3622', 'Hofstader, LEONARD', 'Button', '12/8/2011', '5/25/2011', '12812 N lifty LN', 'SPAYED FEMALE', '$8.00', '4/9/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678']]\n",
      "7_5\n",
      "[['3622', 'Hofstader, LEONARD', 'Button', '12/8/2011', '5/25/2011', '12812 N lifty LN', 'SPAYED FEMALE', '$8.00', '4/9/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678']] [['3622', 'Hofstader, LEONARD', 'Button', '12/8/2011', '5/25/2011', '12812 N lifty LN', 'SPAYED FEMALE', '$8.00', '4/9/2014', 'MEQUON, WI 53092', 'GERMAN W.H. POINTER', '$0.00', 'N/A', '234234', 'LIVER/WHITE', 'CASH', '22678']]\n",
      "17_1\n",
      "[['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']] [['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']]\n",
      "17_2\n",
      "17_3\n",
      "[['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']] [['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']]\n",
      "17_4\n",
      "[['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']] [['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']]\n",
      "17_5\n",
      "[['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']] [['Header1', 'Header3', 'Header5', 'Header2', 'Header4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4'], ['data1', 'data3', 'data5', 'data2', 'data4']]\n",
      "19_1\n",
      "[['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:54', '-89'], ['14:54', '-43'], ['14:54', '-76'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-90'], ['14:55', '-78'], ['14:55', '-76'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-45'], ['14:55', '-97'], ['14:55', '-45'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']] [['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:54', '-89'], ['14:54', '-43'], ['14:54', '-76'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-90'], ['14:55', '-78'], ['14:55', '-76'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-45'], ['14:55', '-97'], ['14:55', '-45'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']]\n",
      "19_2\n",
      "[['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-90'], ['14:55', '-78'], ['14:55', '-76'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-45'], ['14:55', '-97'], ['14:55', '-45'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']] [['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-90'], ['14:55', '-78'], ['14:55', '-76'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-45'], ['14:55', '-97'], ['14:55', '-45'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']]\n",
      "19_3\n",
      "19_4\n",
      "[['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']] [['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']]\n",
      "19_5\n",
      "[['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']] [['14:54', '-72'], ['14:54', '-71'], ['14:54', '-90'], ['14:55', '-45'], ['14:55', '-87'], ['14:55', '-98'], ['14:55', '-87'], ['14:55', '-87'], ['14:55', '-98'], ['14:66', '-76'], ['14:66', '-98'], ['14:66', '-45']]\n",
      "22_1\n",
      "[['book', '5', '6', '7'], ['cat', '2', '3', '8'], ['dog', '2', '3', '4'], ['computer', '0', '6', '1'], ['cell phone', '6', '3', '8']] [['book', '5', '6', '7'], ['cat', '2', '3', '8'], ['dog', '2', '3', '4'], ['computer', '0', '6', '1'], ['cell phone', '6', '3', '8']]\n",
      "22_2\n",
      "[['book', '5', '6', '7'], ['dog', '2', '3', '4'], ['computer', '0', '6', '1'], ['cell phone', '6', '3', '8']] [['book', '5', '6', '7'], ['dog', '2', '3', '4'], ['computer', '0', '6', '1'], ['cell phone', '6', '3', '8']]\n",
      "22_3\n",
      "[['book', '5', '6', '7'], ['dog', '2', '3', '4'], ['cell phone', '6', '3', '8']] [['book', '5', '6', '7'], ['dog', '2', '3', '4'], ['cell phone', '6', '3', '8']]\n",
      "22_4\n",
      "22_5\n",
      "[['book', '5', '6', '7'], ['dog', '2', '3', '4'], ['cell phone', '6', '3', '8']] [['book', '5', '6', '7'], ['dog', '2', '3', '4'], ['cell phone', '6', '3', '8']]\n",
      "26_1\n",
      "[['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name3', 'Age3', 'Gender3', 'Birthday3'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name666', 'Age666', 'Gender666', 'Birthday666'], ['Name555', 'Age555', 'Gender555', 'Birthday555']] [['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name3', 'Age3', 'Gender3', 'Birthday3'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name666', 'Age666', 'Gender666', 'Birthday666'], ['Name555', 'Age555', 'Gender555', 'Birthday555']]\n",
      "26_2\n",
      "[['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name666', 'Age666', 'Gender666', 'Birthday666'], ['Name555', 'Age555', 'Gender555', 'Birthday555']] [['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name666', 'Age666', 'Gender666', 'Birthday666'], ['Name555', 'Age555', 'Gender555', 'Birthday555']]\n",
      "26_3\n",
      "[[['Name2'], ['Age2'], ['Gender2'], ['Birthday2']], [['Name233'], ['Age233'], ['Gender233'], ['Birthday233']], [['Name555'], ['Age555'], ['Gender555'], ['Birthday555']]] [['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name555', 'Age555', 'Gender555', 'Birthday555']]\n",
      "26_4\n",
      "[['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name555', 'Age555', 'Gender555', 'Birthday555']] [['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name555', 'Age555', 'Gender555', 'Birthday555']]\n",
      "26_5\n",
      "[['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name555', 'Age555', 'Gender555', 'Birthday555']] [['Name2', 'Age2', 'Gender2', 'Birthday2'], ['Name233', 'Age233', 'Gender233', 'Birthday233'], ['Name555', 'Age555', 'Gender555', 'Birthday555']]\n",
      "29_1\n",
      "[['D', 'E', 'F', '776,996', '60', '1,532,159', '76', '494,919', '42', '2,804,074', '178'], ['X', 'Y', 'Z', '832,414', '67', '897,949', '63', '712,365', '52', '2,442,728', '182'], ['F', 'Y', 'I', '818,331', '79', '349,399', '12', '552,587', '40', '1,720,317', '131'], ['T', 'B', 'D', '968,331', '43', '717,397', '28', '683,145', '47', '2,368,873', '118'], ['A', 'F', 'K', '454,724', '71', '568,168', '20', '661,672', '62', '1,684,564', '153']] [['DEF', '776,996', '1,532,159', '494,919', '2,804,074', '60', '76', '42', '178'], ['XYZ', '832,414', '897,949', '712,365', '2,442,728', '67', '63', '52', '182'], ['FYI', '818,331', '349,399', '552,587', '1,720,317', '79', '12', '40', '131'], ['TBD', '968,331', '717,397', '683,145', '2,368,873', '43', '28', '47', '118'], ['AFK', '454,724', '568,168', '661,672', '1,684,564', '71', '20', '62', '153']]\n",
      "29_2\n",
      "[['DEF', '776,996', '60', '1,532,159', '76', '494,919', '42', '2,804,074', '178'], ['FYI', '818,331', '79', '349,399', '12', '552,587', '40', '1,720,317', '131'], ['TBD', '968,331', '43', '717,397', '28', '683,145', '47', '2,368,873', '118'], ['AFK', '454,724', '71', '568,168', '20', '661,672', '62', '1,684,564', '153']] [['DEF', '776,996', '1,532,159', '494,919', '2,804,074', '60', '76', '42', '178'], ['FYI', '818,331', '349,399', '552,587', '1,720,317', '79', '12', '40', '131'], ['TBD', '968,331', '717,397', '683,145', '2,368,873', '43', '28', '47', '118'], ['AFK', '454,724', '568,168', '661,672', '1,684,564', '71', '20', '62', '153']]\n",
      "29_3\n",
      "[['DEF', '776,996', '60', '1,532,159', '76', '494,919', '42', '2,804,074', '178'], ['FYI', '818,331', '79', '349,399', '12', '552,587', '40', '1,720,317', '131'], ['AFK', '454,724', '71', '568,168', '20', '661,672', '62', '1,684,564', '153']] [['DEF', '776,996', '1,532,159', '494,919', '2,804,074', '60', '76', '42', '178'], ['FYI', '818,331', '349,399', '552,587', '1,720,317', '79', '12', '40', '131'], ['AFK', '454,724', '568,168', '661,672', '1,684,564', '71', '20', '62', '153']]\n",
      "29_4\n",
      "[['DEF', '776,996', '60', '1,532,159', '76', '494,919', '42', '2,804,074', '178'], ['FYI', '818,331', '79', '349,399', '12', '552,587', '40', '1,720,317', '131'], ['AFK', '454,724', '71', '568,168', '20', '661,672', '62', '1,684,564', '153']] [['DEF', '776,996', '1,532,159', '494,919', '2,804,074', '60', '76', '42', '178'], ['FYI', '818,331', '349,399', '552,587', '1,720,317', '79', '12', '40', '131'], ['AFK', '454,724', '568,168', '661,672', '1,684,564', '71', '20', '62', '153']]\n",
      "29_5\n",
      "[['DEF', '776,996', '60', '1,532,159', '76', '494,919', '42', '2,804,074', '178'], ['FYI', '818,331', '79', '349,399', '12', '552,587', '40', '1,720,317', '131'], ['AFK', '454,724', '71', '568,168', '20', '661,672', '62', '1,684,564', '153']] [['DEF', '776,996', '1,532,159', '494,919', '2,804,074', '60', '76', '42', '178'], ['FYI', '818,331', '349,399', '552,587', '1,720,317', '79', '12', '40', '131'], ['AFK', '454,724', '568,168', '661,672', '1,684,564', '71', '20', '62', '153']]\n",
      "30_1\n",
      "[['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080224', '0.86859', '0.43926', '0.80475'], ['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080225', '0.99163', '0.28901', '0.17321'], ['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080226', '0.84789', '0.31454', '0.54804'], ['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080227', '0.62677', '0.30783', '0.333']] [['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080224', '0.86859', '0.43926', '0.80475'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080226', '0.84789', '0.31454', '0.54804'], ['200080227', '0.62677', '0.30783', '0.333']]\n",
      "30_2\n",
      "[['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080226', '0.84789', '0.31454', '0.54804'], ['200080227', '0.62677', '0.30783', '0.333']] [['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080226', '0.84789', '0.31454', '0.54804'], ['200080227', '0.62677', '0.30783', '0.333']]\n",
      "30_3\n",
      "[['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080227', '0.62677', '0.30783', '0.333']] [['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080227', '0.62677', '0.30783', '0.333']]\n",
      "30_4\n",
      "30_5\n",
      "[['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080227', '0.62677', '0.30783', '0.333']] [['', 'AAA123excel', 'BBB123access', 'CCC123powerpoint'], ['200080222', '0.81181', '0.45774', '0.78874'], ['200080225', '0.99163', '0.28901', '0.17321'], ['200080227', '0.62677', '0.30783', '0.333']]\n",
      "33_1\n",
      "[['Alice', 'Bob'], ['Alice', 'Carol'], ['Alice', 'Dan'], ['Alice', 'James'], ['Alice', 'Michael']] [['Bob', 'Carol', 'Dan', 'James', 'Michael']]\n",
      "33_2\n",
      "[['Bob', 'Bob', 'Dan', 'James', 'Michael']] [['Bob', 'Dan', 'James', 'Michael']]\n",
      "33_3\n",
      "[['Bob', 'Dan', 'Michael']] [['Bob', 'Dan', 'Michael']]\n",
      "33_4\n",
      "['Bob', 'Dan', 'Michael'] [['Bob', 'Dan', 'Michael']]\n",
      "33_5\n",
      "[['Bob', 'Dan', 'Michael']] [['Bob', 'Dan', 'Michael']]\n",
      "34_1\n",
      "34_2\n",
      "[['12345', 'Account1', 'keyword2', 'Not in top 100', '20', 'Not in top 100'], ['12346', 'Account2', 'keyword4', '2', '1', '10'], ['12347', 'Account3', 'keyword5', 'Not in top 100', '35', '8', 'keyword6', 'Not in top 100', '83', '25', 'keyword7', '1', '2', '3']] [['12345', 'Account1', 'keyword2', 'Not in top 100', '20', 'Not in top 100', '', '', '', '', '', '', '', '', ''], ['12346', 'Account2', 'keyword4', '2', '1', '10', '', '', '', '', '', '', '', '', ''], ['12347', 'Account3', 'keyword5', 'Not in top 100', '35', '8', 'keyword6', 'Not in top 100', '83', '25', 'Account3', 'keyword7', '1', '2', '3']]\n",
      "34_3\n",
      "[['12345', 'Account1', 'keyword2', 'Not in top 100', '20', 'Not in top 100', '', '', '', ''], ['12346', 'Account2', 'keyword4', '2', '1', '10', '', '', '', ''], ['12347', 'Account3', 'keyword6', 'Not in top 100', '83', '25', 'keyword7', '1', '2', '3']] [['12345', 'Account1', 'keyword2', 'Not in top 100', '20', 'Not in top 100', '', '', '', ''], ['12346', 'Account2', 'keyword4', '2', '1', '10', '', '', '', ''], ['12347', 'Account3', 'keyword6', 'Not in top 100', '83', '25', 'keyword7', '1', '2', '3']]\n",
      "34_4\n",
      "34_5\n",
      "[['12345', 'Account1', 'keyword2', 'Not in top 100', '20', 'Not in top 100', '', '', '', ''], ['12346', 'Account2', 'keyword4', '2', '1', '10', '', '', '', ''], ['12347', 'Account3', 'keyword6', 'Not in top 100', '83', '25', '', '', '', '']] [['12345', 'Account1', 'keyword2', 'Not in top 100', '20', 'Not in top 100'], ['12346', 'Account2', 'keyword4', '2', '1', '10'], ['12347', 'Account3', 'keyword6', 'Not in top 100', '83', '25']]\n",
      "37_1\n",
      "[['Last, First middle'], ['Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter', 'Last, First middle'], ['Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter', 'Last, First middle'], ['Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter', 'Last, First middle'], ['Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter', 'Last, First middle'], ['Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']] [['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']]\n",
      "37_2\n",
      "37_3\n",
      "[['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']] [['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']]\n",
      "37_4\n",
      "[['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']] [['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter'], ['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']]\n",
      "37_5\n",
      "[['Last, First middle', 'Organization']] [['Last, First middle', 'Organization', 'Position', 'Rank', 'City', 'State/Province', 'Phone', 'Fax', 'Email', 'Chapter']]\n",
      "40_1\n",
      "[['123', 'sod', 'spouse', '38'], ['123', 'dos', 'son', '18'], ['544', 'doj', 'daughter', '20'], ['270', 'doa', 'brother', '50'], ['981', 'don', 'daughter', '16']] [['123', 'sod', 'spouse', '38', '123', 'dos', 'son', '18', '544', 'doj', 'daughter', '20', '270', 'doa', 'brother', '50', '981', 'don', 'daughter', '16']]\n",
      "40_2\n",
      "[['123', 'sod', 'spouse', '38'], ['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20'], ['123', 'sod', 'spouse', '38', '270', 'doa', 'brother', '50'], ['123', 'sod', 'spouse', '38', '981', 'don', 'daughter', '16']] [['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '270', 'doa', 'brother', '50', '981', 'don', 'daughter', '16']]\n",
      "40_3\n",
      "[['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '981', 'don', 'daughter', '16']] [['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '981', 'don', 'daughter', '16']]\n",
      "40_4\n",
      "[['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '981', 'don', 'daughter', '16']] [['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '981', 'don', 'daughter', '16']]\n",
      "40_5\n",
      "[['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '981', 'don', 'daughter', '16']] [['123', 'sod', 'spouse', '38', '544', 'doj', 'daughter', '20', '981', 'don', 'daughter', '16']]\n",
      "43_1\n",
      "43_2\n",
      "[['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Beard, Brian K.', 'Partner', 'Austin', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']] [['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Beard, Brian K.', 'Partner', 'Austin', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']]\n",
      "43_3\n",
      "[['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']] [['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']]\n",
      "43_4\n",
      "[[['Anthony, Scott A.'], ['Partner'], ['Palo Alto'], ['Mergers & Acquisitions']], [['Baudler, Mark B.'], ['Partner'], ['Palo Alto'], ['Mergers & Acquisitions']], [['Berger, David J.'], ['Partner'], ['San Francisco, Palo Alto'], ['Mergers & Acquisitions']]] [['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']]\n",
      "43_5\n",
      "[['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']] [['Anthony, Scott A.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Baudler, Mark B.', 'Partner', 'Palo Alto', 'Mergers & Acquisitions'], ['Berger, David J.', 'Partner', 'San Francisco, Palo Alto', 'Mergers & Acquisitions']]\n",
      "44_1\n",
      "[['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']] [['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']]\n",
      "44_2\n",
      "[['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']] [['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']]\n",
      "44_3\n",
      "[['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']] [['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']]\n",
      "44_4\n",
      "[['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']] [['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']]\n",
      "44_5\n",
      "[['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']] [['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:'], ['Last Name:', 'First Name:', 'Card Number:', 'Badge Expiration Date:', 'Status:']]\n",
      "46_1\n",
      "[['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['4', 'lkj', 'd', 'wer', 'g', 'xcv', 'h'], ['7', 'erf', 'e'], ['8', 'rdf', 's', 'asd', 'h'], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd']] [['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['4', 'lkj', 'd', 'wer', 'g', 'xcv', 'h', '', '', '', ''], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['8', 'rdf', 's', 'asd', 'h', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']]\n",
      "46_2\n",
      "[['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y', ''], ['7', 'erf', 'e', '', '', '', '', '', '', '', '', ''], ['8', 'rdf', 's', 'asd', 'h', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '', '']] [['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['8', 'rdf', 's', 'asd', 'h', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']]\n",
      "46_3\n",
      "[['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']] [['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']]\n",
      "46_4\n",
      "[['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']] [['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']]\n",
      "46_5\n",
      "[['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']] [['2', 'ghi', 'x', 'abc', 'n', 'lmn', 'x', 'def', 'z', 'jkl', 'y'], ['7', 'erf', 'e', '', '', '', '', '', '', '', ''], ['9', 'qew', 'u', 'iew', 'i', 'ney', 'd', '', '', '', '']]\n",
      "47_1\n",
      "[['F', 'G', 'H', 'I', 'JJ', 'KK', 'L', 'MM', 'N', 'O', 'II', 'L', 'Y', 'B', 'W', 'P', 'O', 'N', 'Q', 'HH', 'DD', 'LL', 'U', 'AA', 'W']] [['F', 'G', 'H', 'I', 'JJ'], ['KK', 'L', 'MM', 'N', 'O'], ['II', 'L', 'Y', 'B', 'W'], ['P', 'O', 'N', 'Q', 'HH'], ['DD', 'LL', 'U', 'AA', 'W']]\n",
      "47_2\n",
      "[['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['P', 'O', 'N', 'Q', 'HH'], ['DD', 'LL', 'U', 'AA', 'W']] [['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['P', 'O', 'N', 'Q', 'HH'], ['DD', 'LL', 'U', 'AA', 'W']]\n",
      "47_3\n",
      "[['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['DD', 'LL', 'U', 'AA', 'W']] [['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['DD', 'LL', 'U', 'AA', 'W']]\n",
      "47_4\n",
      "[['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['DD', 'LL', 'U', 'AA', 'W']] [['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['DD', 'LL', 'U', 'AA', 'W']]\n",
      "47_5\n",
      "[['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['DD', 'LL', 'U', 'AA', 'W']] [['F', 'G', 'H', 'I', 'JJ'], ['II', 'L', 'Y', 'B', 'W'], ['DD', 'LL', 'U', 'AA', 'W']]\n",
      "potters_wheel_unfold_1\n",
      "[['', 'English', 'French', 'Math'], ['Anna', '', '78', '43'], ['Bob', '96', '54', ''], ['Joan', '79', '', ''], ['Rob', '87', '92', ''], ['Tom', '', '85', '90']] [['', 'Math', 'French', 'English'], ['Anna', '43', '78', ''], ['Bob', '', '54', '96'], ['Joan', '', '', '79'], ['Tom', '90', '85', ''], ['Rob', '', '92', '87']]\n",
      "potters_wheel_unfold_2\n",
      "[['', 'English', 'French', 'Math'], ['Anna', '', '78', '43'], ['Bob', '96', '54', ''], ['Tom', '', '85', '90'], ['Rob', '87', '92', '']] [['', 'Math', 'French', 'English'], ['Anna', '43', '78', ''], ['Bob', '', '54', '96'], ['Tom', '90', '85', ''], ['Rob', '', '92', '87']]\n",
      "potters_wheel_unfold_3\n",
      "potters_wheel_unfold_4\n",
      "[['', 'Math', 'French', 'English'], ['Anna', '43', '78', ''], ['Tom', '90', '85', '']] [['', 'Math', 'French'], ['Anna', '43', '78'], ['Tom', '90', '85']]\n",
      "potters_wheel_unfold_5\n",
      "[['', 'Math', 'French', 'English'], ['Anna', '43', '78', ''], ['Tom', '90', '85', '']] [['', 'Math', 'French'], ['Anna', '43', '78'], ['Tom', '90', '85']]\n",
      "proactive_wrangling_complex_1\n",
      "[['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Frank K.', '(615)564-6500', '(615)564-6701'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Robert B.', '(517)555-0127', '(517)555-0142'], ['Tywin L.', '(518)555-0103', '(518)555-0111']] [['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Frank K.', '(615)564-6500', '(615)564-6701'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Robert B.', '(517)555-0127', '(517)555-0142'], ['Tywin L.', '(518)555-0103', '(518)555-0111']]\n",
      "proactive_wrangling_complex_2\n",
      "[['', 'Tel', 'Fax'], ['Bureau of I.A.', '', ''], ['Regional Director', '', ''], ['Jean H.', '', '(918)781-4604'], ['Eddard S.', '', '(404)555-0139'], ['Robert B.', '', '(517)555-0142'], ['Tywin L.', '', '(518)555-0111']] [['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Robert B.', '(517)555-0127', '(517)555-0142'], ['Tywin L.', '(518)555-0103', '(518)555-0111']]\n",
      "proactive_wrangling_complex_3\n",
      "[['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Tywin L.', '(518)555-0103', '(518)555-0111']] [['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Tywin L.', '(518)555-0103', '(518)555-0111']]\n",
      "proactive_wrangling_complex_4\n",
      "[['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Tywin L.', '(518)555-0103', '(518)555-0111']] [['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Tywin L.', '(518)555-0103', '(518)555-0111']]\n",
      "proactive_wrangling_complex_5\n",
      "[['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Tywin L.', '(518)555-0103', '(518)555-0111']] [['', 'Tel', 'Fax'], ['Jean H.', '(918)781-4600', '(918)781-4604'], ['Eddard S.', '(404)555-0121', '(404)555-0139'], ['Tywin L.', '(518)555-0103', '(518)555-0111']]\n"
     ]
    }
   ],
   "source": [
    "acc_result = calculate_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'2': [['2_1', 0.42857142857142855],\n",
       "              ['2_2', 0],\n",
       "              ['2_3', 0],\n",
       "              ['2_4', 0.5714285714285714],\n",
       "              ['2_5', 0.5714285714285714]],\n",
       "             '7': [['7_1', 0],\n",
       "              ['7_2', 1.0],\n",
       "              ['7_3', 0],\n",
       "              ['7_4', 1.0],\n",
       "              ['7_5', 1.0]],\n",
       "             '17': [['17_1', 1.0],\n",
       "              ['17_2', 0],\n",
       "              ['17_3', 1.0],\n",
       "              ['17_4', 1.0],\n",
       "              ['17_5', 1.0]],\n",
       "             '19': [['19_1', 1.0],\n",
       "              ['19_2', 1.0],\n",
       "              ['19_3', 0],\n",
       "              ['19_4', 1.0],\n",
       "              ['19_5', 1.0]],\n",
       "             '22': [['22_1', 1.0],\n",
       "              ['22_2', 1.0],\n",
       "              ['22_3', 1.0],\n",
       "              ['22_4', 0],\n",
       "              ['22_5', 1.0]],\n",
       "             '26': [['26_1', 1.0],\n",
       "              ['26_2', 1.0],\n",
       "              ['26_3', 0.0],\n",
       "              ['26_4', 1.0],\n",
       "              ['26_5', 1.0]],\n",
       "             '29': [['29_1', 0.1111111111111111],\n",
       "              ['29_2', 0.3333333333333333],\n",
       "              ['29_3', 0.3333333333333333],\n",
       "              ['29_4', 0.3333333333333333],\n",
       "              ['29_5', 0.3333333333333333]],\n",
       "             '30': [['30_1', 0.3333333333333333],\n",
       "              ['30_2', 1.0],\n",
       "              ['30_3', 1.0],\n",
       "              ['30_4', 0],\n",
       "              ['30_5', 1.0]],\n",
       "             '33': [['33_1', 0.0],\n",
       "              ['33_2', 0.25],\n",
       "              ['33_3', 1.0],\n",
       "              ['33_4', 0.0],\n",
       "              ['33_5', 1.0]],\n",
       "             '34': [['34_1', 0],\n",
       "              ['34_2', 0.4888888888888889],\n",
       "              ['34_3', 1.0],\n",
       "              ['34_4', 0],\n",
       "              ['34_5', 1.0]],\n",
       "             '37': [['37_1', 0.02],\n",
       "              ['37_2', 0],\n",
       "              ['37_3', 1.0],\n",
       "              ['37_4', 1.0],\n",
       "              ['37_5', 0.2]],\n",
       "             '40': [['40_1', 0.2],\n",
       "              ['40_2', 0.25],\n",
       "              ['40_3', 1.0],\n",
       "              ['40_4', 1.0],\n",
       "              ['40_5', 1.0]],\n",
       "             '43': [['43_1', 0],\n",
       "              ['43_2', 1.0],\n",
       "              ['43_3', 1.0],\n",
       "              ['43_4', 0.0],\n",
       "              ['43_5', 1.0]],\n",
       "             '44': [['44_1', 1.0],\n",
       "              ['44_2', 1.0],\n",
       "              ['44_3', 1.0],\n",
       "              ['44_4', 1.0],\n",
       "              ['44_5', 1.0]],\n",
       "             '46': [['46_1', 0.6],\n",
       "              ['46_2', 1.0],\n",
       "              ['46_3', 1.0],\n",
       "              ['46_4', 1.0],\n",
       "              ['46_5', 1.0]],\n",
       "             '47': [['47_1', 0.2],\n",
       "              ['47_2', 1.0],\n",
       "              ['47_3', 1.0],\n",
       "              ['47_4', 1.0],\n",
       "              ['47_5', 1.0]],\n",
       "             'potters_wheel_unfold': [['potters_wheel_unfold_1',\n",
       "               0.4166666666666667],\n",
       "              ['potters_wheel_unfold_2', 0.5],\n",
       "              ['potters_wheel_unfold_3', 0],\n",
       "              ['potters_wheel_unfold_4', 1.0],\n",
       "              ['potters_wheel_unfold_5', 1.0]],\n",
       "             'proactive_wrangling_complex': [['proactive_wrangling_complex_1',\n",
       "               1.0],\n",
       "              ['proactive_wrangling_complex_2', 0.2],\n",
       "              ['proactive_wrangling_complex_3', 1.0],\n",
       "              ['proactive_wrangling_complex_4', 1.0],\n",
       "              ['proactive_wrangling_complex_5', 1.0]]})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gpt_date_acc_result.json', \"w\") as f:\n",
    "    json.dump(acc_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_acc = []\n",
    "for key in acc_result:\n",
    "    acc = 0\n",
    "    for i in acc_result[key]:\n",
    "        acc += i[1]\n",
    "    overall_acc.append([key, acc/len(acc_result[key])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'2': [0.3142857142857143],\n",
       "             '7': [0.6],\n",
       "             '17': [0.8],\n",
       "             '19': [0.8],\n",
       "             '22': [0.8],\n",
       "             '26': [0.8],\n",
       "             '29': [0.28888888888888886],\n",
       "             '30': [0.6666666666666666],\n",
       "             '33': [0.45],\n",
       "             '34': [0.49777777777777776],\n",
       "             '37': [0.44400000000000006],\n",
       "             '40': [0.6900000000000001],\n",
       "             '43': [0.6],\n",
       "             '44': [1.0],\n",
       "             '46': [0.9199999999999999],\n",
       "             '47': [0.8400000000000001],\n",
       "             'potters': [0.5833333333333334],\n",
       "             'proactive': [0.8400000000000001]})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_acc = defaultdict(list)\n",
    "for i in overall_acc:\n",
    "    exp = i[0].split('_')[0]\n",
    "    list_acc[exp].append(i[1])\n",
    "list_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.3142857142857143\n",
      "7 0.6\n",
      "17 0.8\n",
      "19 0.8\n",
      "22 0.8\n",
      "26 0.8\n",
      "29 0.28888888888888886\n",
      "30 0.6666666666666666\n",
      "33 0.45\n",
      "34 0.49777777777777776\n",
      "37 0.44400000000000006\n",
      "40 0.6900000000000001\n",
      "43 0.6\n",
      "44 1.0\n",
      "46 0.9199999999999999\n",
      "47 0.8400000000000001\n",
      "potters 0.5833333333333334\n",
      "proactive 0.8400000000000001\n"
     ]
    }
   ],
   "source": [
    "for e in list_acc:\n",
    "    print(e, sum(list_acc[e])/len(list_acc[e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
